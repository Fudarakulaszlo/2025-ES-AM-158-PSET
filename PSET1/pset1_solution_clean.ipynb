{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c41cb8",
   "metadata": {},
   "source": [
    "# Problem Set 1 — Three Problems in Dynamic Programming\n",
    "\n",
    "*Markov Decision Processes & DP Methods*\n",
    "\n",
    "### Problem 1 — Inscribed Polygon of Maximum Perimeter (Pen & Paper)\n",
    "TODO list:\n",
    "- (a) show Q-value function $Q_{N-1}$ (6pt)\n",
    "- (b) show convexity (6pt)\n",
    "- (c) show optimal control signal $u_{N-1}$ (6pt)\n",
    "- (d) induction to any $k$-th step Q-function $Q_k$ (6pt)\n",
    "- (e) show all optimal control signal $u_k$ (6pt)\n",
    "\n",
    "Bonus:\n",
    "- (f) show convexity (5pt)\n",
    "- (g)(coding) solve the problem using optimization (5pt)\n",
    "### Problem 2 — Proof of convergence of value iteration (Pen & Paper)\n",
    "TODO list:\n",
    "- 2.1 contraction of bellman operator (5pt)\n",
    "- 2.2 linear convergence (5pt)\n",
    "- 2.3 stoping criteria (5pt)\n",
    "- 2.4 iteration bound (5pt)\n",
    "### Problem 3 — Cliffwalk (coding)\n",
    "TODO list:\n",
    "- 3.2 fill in code for policy evaluation (10pt)\n",
    "- 3.3 fill in code for policy iteration (10pt)\n",
    "- 3.4 fill in code for value iteration (10pt)\n",
    "\n",
    "### Problem 4 — Matrix–Vector Representation of DP\n",
    "TODO list:\n",
    "- 4.1 build the transition matrix $P$ (5pt)\n",
    "- 4.2 write bellman equation as matrix form (5pt)\n",
    "- 4.3 solve the matrix equation by fix-point iteration (10pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999f44b8",
   "metadata": {},
   "source": [
    "## 1. Inscribed Polygon of Maximal Perimeter (Pen and Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd1ea",
   "metadata": {},
   "source": [
    "In lectures, we have seen how dynamic programming (DP) can compute optimal value functions and optimal policies for finite-horizon MDPs with discrete state space and action space (i.e., the tabular case).\n",
    "\n",
    "In this exercise, we will see that DP can also solve an optimal control problem with continuous state space and action space.\n",
    "This problem is a geometry problem where we try to find the $N$-side polygon inscribed inside a circle with maximum perimeter. We will walk you through the key steps of formulating and solving the problem, while leaving a few mathematical details for you to fill in.\n",
    "\n",
    "Given a circle with radius $1$, we can randomly choose $N$ distinct points on the circle to form a polygon with $N$ vertices and sides, as shown in Fig. 1 with $N=3,4,5$.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/polygon-inside-circle.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 1. Polygons inscribed inside a circle\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Once the $N$ points are chosen, the $N$-polygon will have a perimeter, i.e., the sum of the lengths of its edges.\n",
    "\n",
    "What is the configuration of the $N$ points such that the resulting $N$-polygon has the maximum perimeter? We claim that the answer is when the $N$-polygon has edges of equal lengths, or in other words, when the $N$ points are placed on the circle evenly.\n",
    "\n",
    "Let us use dynamic programming to prove the claim.\n",
    "\n",
    "To use dynamic programming, we need to define a dynamical system and a reward function.\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/sequential-placement-N-point.png\" width=\"360\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 2. Sequential placement of N points on the circle.\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "**Dynamical system.**\n",
    "\n",
    "\n",
    "We will use $\\{x_1, \\ldots, x_N\\}$ to denote the angular positions of the $N$ points to be placed on the circle (with slight abuse of notation, we will call each of those points $x_k$ as well). In particular, as shown in Fig. 2, let us use $x_k$ to denote the angle between the line $O — x_k$ and the vertical line (O is the center of the circle), with zero angle starting at 12 o’clock and clockwise being positive. Without loss of generality, we assume $x_1 = 0$. (if $x_1$ is nonzero, we can always rotate the entire circle so that $x_1 = 0$).\n",
    "\n",
    "After the $k$-th point is placed, we can “control” where the next point $x_{k+1}$ will be, by deciding the incremental angle between $x_{k+1}$ and $x_k$, denoted as $u_k > 0$ in Fig. 2. This is simply saying the dynamics is\n",
    "\n",
    "$$\n",
    "x_{k+1} = x_k + u_k, \\quad k=1,\\ldots,N-1, \\quad x_1 = 0.\n",
    "$$\n",
    "\n",
    "Notice here we did not use an MDP to formulate this problem because the dynamics is deterministic. In the MDP language, this would correspond to, at time step $k$, if the agent takes action $u_k$ at state $x_k$, then the probability of transitioning to state $x_k + u_k$ at time $k+1$ is $1$, and the probability of transitioning to other states is zero.\n",
    "\n",
    "\n",
    "**Reward.**\n",
    "\n",
    "\n",
    "The perimeter of the $N$-polygon is therefore\n",
    "\n",
    "$$\n",
    "g_N(x_N) + \\sum_{k=1}^{N-1} g_k(x_k,u_k),\n",
    "$$\n",
    "\n",
    "with the terminal reward\n",
    "\n",
    "$$\n",
    "g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right),\n",
    "$$\n",
    "\n",
    "the distance between $x_N$ and $x_1$ (see Fig. 2), and the running reward\n",
    "\n",
    "$$\n",
    "g_k(x_k,u_k) = 2 \\sin\\left(\\frac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "**Dynamic programming.**\n",
    "\n",
    "We are now ready to invoke dynamic programming. Recall in lectures the key steps of DP are first to initialize the optimal value functions at the terminal time $k=N$, and then perform backward recursion to compute the optimal value functions at time $k=N-1,\\dots,1$.\n",
    "\n",
    "We start by setting\n",
    "\n",
    "$$\n",
    "V_N(x_N) = g_N(x_N) = 2 \\sin\\left(\\frac{2\\pi-x_N}{2}\\right).\n",
    "$$\n",
    "\n",
    "Unlike in lectures where we initialized the terminal value functions as all zero, here we initialize the terminal value functions as $g_N(x_N)$ because there is a \"terminal-state\" reward.\n",
    "\n",
    "We then compute $V_{N-1}(x_{N-1})$ as\n",
    "\n",
    "$$\n",
    "V_{N-1}(x_{N-1})\n",
    "= \\max_{0 < u_{N-1} < 2\\pi-x_{N-1}}\n",
    "\\left\\{\n",
    "  \\underbrace{ 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right) + V_N(x_{N-1} + u_{N-1}) }_{Q_{N-1}(x_{N-1}, u_{N-1})}\n",
    "\\right\\},\n",
    "\\tag{9.1}\n",
    "$$\n",
    "\n",
    "where $u_{N-1} < 2\\pi-x_{N-1}$ because we do not want $x_N$ to cross $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0764d9",
   "metadata": {},
   "source": [
    "\n",
    "**a**. Show that\n",
    "\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right),\n",
    "$$\n",
    "\n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Q_{N-1}(x_{N-1}, u_{N-1})}{\\partial u_{N-1}}\n",
    "= \\cos\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "- \\cos\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "By definition,\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1}) = g_{N-1}(x_{N-1}, u_{N-1}) + V_N(x_N) = g_{N-1}(x_{N-1}, u_{N-1}) + V_N(x_{N-1} + u_{N-1})\n",
    "$$\n",
    "So gives us\n",
    "$$\n",
    "Q_{N-1}(x_{N-1}, u_{N-1})\n",
    "= 2 \\sin\\left(\\tfrac{u_{N-1}}{2}\\right)\n",
    "+ 2 \\sin\\left(\\tfrac{2\\pi-x_{N-1}-u_{N-1}}{2}\\right)\n",
    "$$\n",
    "Differentiating w.r.t. $u_{N-1}$,\n",
    "$$\n",
    "\\frac{\\partial Q_{N-1}}{\\partial u_{N-1}}\n",
    "= \\cos\\!\\Big(\\frac{u_{N-1}}{2}\\Big)\n",
    " - \\cos\\!\\Big(\\frac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a666ead0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**b**. Show that $Q_{N-1}(x_{N-1}, u_{N-1})$ is concave (i.e., $-Q_{N-1}(x_{N-1}, u_{N-1})$ is convex) in $u_{N-1}$ for every $x_{N-1} \\in (0, \\pi)$ and $u_{N-1} \\in (0, 2\\pi-x_{N-1})$.\n",
    "(Hint: compute the second derivative of $-Q_{N-1}(x_{N-1}, u_{N-1})$ with respect to $u_{N-1}$ and verify it is positive definite)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "Second derivative:\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}}{\\partial u_{N-1}^2}\n",
    "= -\\frac{1}{2}\\sin\\!\\Big(\\frac{u_{N-1}}{2}\\Big)\n",
    "  -\\frac{1}{2}\\sin\\!\\Big(\\frac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big).\n",
    "$$\n",
    "For $x_{N-1}\\in(0,\\pi)$ and $u_{N-1}\\in(0,2\\pi-x_{N-1})$, both arguments of $\\sin(\\cdot)$ lie in $(0,\\pi)$, hence the sines are strictly positive and the sum is positive. Therefore\n",
    "$$\n",
    "\\frac{\\partial^2 Q_{N-1}}{\\partial u_{N-1}^2} < 0,\n",
    "$$\n",
    "so $Q_{N-1}$ is **strictly concave** in $u_{N-1}$ on the feasible interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71bd6ca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**c**. With a and b, show that the optimal $u_{N-1}$ that solves (9.1) is\n",
    "\n",
    "$$\n",
    "u_{N-1}^\\star = \\frac{2\\pi-x_{N-1}}{2},\n",
    "$$\n",
    "\n",
    "and therefore\n",
    "\n",
    "$$\n",
    "J_{N-1}(x_{N-1}) = 4 \\sin\\left(\\tfrac{2\\pi-x_{N-1}}{4}\\right).\n",
    "$$\n",
    "\n",
    "(Hint: the point at which a concave function’s gradient vanishes must be the unique maximizer of that function.)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "Strict concavity implies the unique maximizer satisfies the first-order condition:\n",
    "$$\n",
    "\\cos\\!\\Big(\\frac{u_{N-1}}{2}\\Big)\n",
    "= \\cos\\!\\Big(\\frac{2\\pi - x_{N-1} - u_{N-1}}{2}\\Big)\n",
    "\\;\\Longrightarrow\\;\n",
    "u_{N-1}^\\star = \\frac{2\\pi - x_{N-1}}{2}.\n",
    "$$\n",
    "Evaluating $Q_{N-1}$ at $u_{N-1}^\\star$ gives\n",
    "$$\n",
    "J_{N-1}(x_{N-1})\n",
    "= 2\\sin\\!\\Big(\\frac{u_{N-1}^\\star}{2}\\Big)\n",
    "+ 2\\sin\\!\\Big(\\frac{2\\pi - x_{N-1} - u_{N-1}^\\star}{2}\\Big)\n",
    "= 4 \\sin\\!\\Big(\\frac{2\\pi - x_{N-1}}{4}\\Big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c9458",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**d**. Now use induction to show that the $k$-th step dynamic programming\n",
    "\n",
    "$$\n",
    "J_k(x_k)\n",
    "= \\max_{0 < u_k < 2\\pi — x_k}\n",
    "\\left\\{ 2 \\sin\\left(\\tfrac{u_k}{2}\\right) + J_{k+1}(x_k + u_k) \\right\\}\n",
    "$$\n",
    "\n",
    "admits an optimal control\n",
    "\n",
    "$$\n",
    "u_k^\\star = \\frac{2\\pi-x_k}{N-k + 1},\n",
    "$$\n",
    "\n",
    "and optimal cost-to-go\n",
    "\n",
    "$$\n",
    "J_k(x_k) = 2 (N-k + 1) \\, \\sin\\!\\left( \\frac{2\\pi-x_k}{2 (N-k + 1)} \\right).\n",
    "$$\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "\n",
    "**Proof by induction.** The base case $k=N-1$ holds by part (c). Assume the formula holds for $k+1$:\n",
    "$$\n",
    "J_{k+1}(x)=2(N-k)\\,\\sin\\!\\Big(\\frac{2\\pi - x}{2(N-k)}\\Big).\n",
    "$$\n",
    "Then\n",
    "$$\n",
    "J_k(x)=\\max_{0<u<2\\pi-x}\n",
    "\\left\\{\n",
    "2\\sin\\!\\Big(\\frac{u}{2}\\Big)\n",
    "+2(N-k)\\sin\\!\\Big(\\frac{2\\pi - x - u}{2(N-k)}\\Big)\n",
    "\\right\\}.\n",
    "$$\n",
    "Let\n",
    "$$\n",
    "F(u)\n",
    "=2\\sin\\!\\Big(\\frac{u}{2}\\Big)\n",
    "+2(N-k)\\,\\sin\\!\\Big(\\frac{2\\pi-x-u}{2(N-k)}\\Big),\n",
    "\\qquad 0<u<2\\pi-x,\\ \\ N-k\\ge 1.\n",
    "$$\n",
    "\n",
    "Define\n",
    "$$\n",
    "z(u)=\\frac{2\\pi-x-u}{2(N-k)}.\n",
    "$$\n",
    "Using the chain rule:\n",
    "$$\n",
    "F'(u)\n",
    "=\\frac{d}{du}\\Big[2\\sin\\!\\Big(\\frac{u}{2}\\Big)\\Big]\n",
    "+\\frac{d}{du}\\Big[2(N-k)\\sin\\big(z(u)\\big)\\Big]\n",
    "=\\cos\\!\\Big(\\frac{u}{2}\\Big)-\\cos\\!\\big(z(u)\\big),\n",
    "$$\n",
    "since $z'(u)=-\\tfrac{1}{2(N-k)}$. And thus the second derivative:\n",
    "$$\n",
    "F''(u)\n",
    "=-\\frac{1}{2}\\sin\\!\\Big(\\frac{u}{2}\\Big)\n",
    "-\\frac{1}{2(N-k)}\\sin\\!\\Big(\\frac{2\\pi-x-u}{2(N-k)}\\Big)\n",
    "$$\n",
    "\n",
    "For $0<u<2\\pi-x$,\n",
    "- $\\dfrac{u}{2}\\in(0,\\pi-\\tfrac{x}{2})\\subset(0,\\pi)\\Rightarrow \\sin\\!\\big(\\tfrac{u}{2}\\big)>0$.\n",
    "- $\\dfrac{2\\pi-x-u}{2(N-k)}\\in\\Big(0,\\dfrac{2\\pi-x}{2(N-k)}\\Big)\\subset(0,\\pi)\\Rightarrow \\sin\\!\\Big(\\dfrac{2\\pi-x-u}{2(N-k)}\\Big)>0$\n",
    "  (strictly positive since $u>0$).\n",
    "\n",
    "Both sine terms are positive, and both coefficients are negative, hence\n",
    "$$\n",
    "F''(u)<0\\quad\\text{for }u\\in(0,2\\pi-x).\n",
    "$$\n",
    "\n",
    "Therefore, the objective $F(u)$ is **strictly concave** in $u$ over the feasible set.\n",
    "\n",
    "The FOC yields\n",
    "$$\n",
    "\\cos\\!\\Big(\\frac{u}{2}\\Big)\n",
    "=\\cos\\!\\Big(\\frac{2\\pi - x - u}{2(N-k)}\\Big)\n",
    "\\;\\Longrightarrow\\;\n",
    "u_k^\\star=\\frac{2\\pi - x}{N-k+1}.\n",
    "$$\n",
    "At this $u_k^\\star$, the two sine arguments coincide, so\n",
    "$$\n",
    "J_k(x)= 2\\big(N-k+1\\big)\\,\n",
    "\\sin\\!\\Big(\\frac{2\\pi - x}{2(N-k+1)}\\Big),\n",
    "$$\n",
    "completing the induction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8ec40",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**e**. Starting from $x_1 = 0$, what is the optimal sequence of controls?\n",
    "\n",
    "Hopefully now you see why my original claim is true!\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "\n",
    "Starting from $x_1=0$, the optimal control sequence is\n",
    "$$\n",
    "u_k^\\star=\\frac{2\\pi - x_k}{N-k+1},\\qquad k=1,\\dots,N-1.\n",
    "$$\n",
    "Unrolling, one finds $x_k=\\tfrac{2\\pi(k-1)}{N}$ and therefore\n",
    "$$\n",
    "u_k^\\star=\\frac{2\\pi}{N}\\quad\\text{for all }k=1,\\dots,N-1.\n",
    "$$\n",
    "Hence the $N$ points are evenly spaced and the optimal polygon is the regular $N$-gon. The maximal perimeter equals\n",
    "$$\n",
    "J_1(0)=2N\\sin\\!\\Big(\\frac{\\pi}{N}\\Big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfd1d1",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "We are not yet done for this exercise. Since you have probably already spent quite some time on this exercise, I will leave the rest of the exercise a bonus. In case you found this simple geometric problem interesting, you should keep reading as we will use numerical techniques to prove the same claim.\n",
    "\n",
    "In Fig. 2, by denoting\n",
    "\n",
    "$$\n",
    "u_N = 2\\pi - x_N = 2\\pi - (u_1 + \\cdots + u_{N-1})\n",
    "$$\n",
    "\n",
    "as the angle between the line $O — x_N$ and the line $O — x_1$, it is not hard to observe that the perimeter of the $N$-polygon is\n",
    "\n",
    "$$\n",
    "\\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right).\n",
    "$$\n",
    "\n",
    "Consequently, to maximize the perimeter, we can formulate the following optimization\n",
    "\n",
    "$$\n",
    "\\max_{u_1,\\ldots,u_N} \\;\\; \\sum_{k=1}^N 2 \\sin\\!\\left(\\tfrac{u_k}{2}\\right)\n",
    "$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$\n",
    "u_k > 0, \\; k = 1, \\ldots, N, \\\\\n",
    "u_1 + \\cdots + u_N = 2\\pi\n",
    "\\tag{9.2}\n",
    "$$\n",
    "\n",
    "where $u_k$ can be seen as the angle spanned by the line $x_k — x_{k+1}$ with respect to the center $O$ so that they are positive and sum up to $2\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21f4bb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**f**. Show that the optimization (9.2) is convex.\n",
    "(Hint: first show the feasible set is convex, and then show the objective function is concave over the feasible set.)\n",
    "\n",
    "**(TODO) ANSWER:**\n",
    "\n",
    "**Feasible set is convex**\n",
    "\n",
    "Let\n",
    "$$\n",
    "\\mathcal{F}=\\Big\\{u\\in\\mathbb{R}^N:\\ u_k>0\\ \\forall k,\\ \\mathbf{1}^\\top u=2\\pi\\Big\\}.\n",
    "$$\n",
    "Take any $u,v\\in\\mathcal{F}$ and any $\\theta\\in(0,1)$. Then\n",
    "- Positivity: $(\\theta u+(1-\\theta)v)_k=\\theta u_k+(1-\\theta)v_k>0$.\n",
    "- Sum constraint: $\\mathbf{1}^\\top(\\theta u+(1-\\theta)v)=\\theta\\,\\mathbf{1}^\\top u+(1-\\theta)\\,\\mathbf{1}^\\top v=\\theta(2\\pi)+(1-\\theta)(2\\pi)=2\\pi$.\n",
    "\n",
    "Hence $\\theta u+(1-\\theta)v\\in\\mathcal{F}$. Therefore $\\mathcal{F}$ is convex.\n",
    "\n",
    "**Objective is concave on the feasible set**\n",
    "\n",
    "Define the scalar function\n",
    "$$\n",
    "\\phi(t)=2\\sin\\!\\left(\\frac{t}{2}\\right),\\qquad t\\in(0,2\\pi).\n",
    "$$\n",
    "Compute derivatives:\n",
    "$$\n",
    "\\phi'(t)=\\cos\\!\\left(\\frac{t}{2}\\right),\\qquad\n",
    "\\phi''(t)=-\\tfrac{1}{2}\\sin\\!\\left(\\frac{t}{2}\\right).\n",
    "$$\n",
    "For $t\\in(0,2\\pi)$, we have $t/2\\in(0,\\pi)$, so $\\sin(t/2)>0$. Therefore\n",
    "$$\n",
    "\\phi''(t)=-\\tfrac{1}{2}\\sin\\!\\left(\\tfrac{t}{2}\\right)<0,\n",
    "$$\n",
    "which shows $\\phi$ is **strictly concave** on $(0,2\\pi)$.\n",
    "\n",
    "The objective is a sum of concave functions:\n",
    "$$\n",
    "f(u)=\\sum_{k=1}^N \\phi(u_k),\n",
    "$$\n",
    "so $f$ is concave on $(0,2\\pi)^N$. Restricting $f$ to the convex feasible set $\\mathcal{F}$ preserves concavity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee5a38",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have shown (9.2) is a convex optimization problem, we know that pretty much any numerical algorithm will guarantee convergence to the globally optimal solution.\n",
    "\n",
    "There are many numerical algorithms that can compute optimal solutions of an optimization problem (Nocedal and Wright 1999). Python provides a nice interface, `scipy.optimize`, to many such algorithms, and let us use `scipy.optimize` to solve (9.2) so we can numerically prove our claim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d86a56",
   "metadata": {},
   "source": [
    "\n",
    "**g**. We have provided most of the code necessary for solving (9.2) below. Please fill in the definition of the function `perimeter(u)`, and then run the code. Show your results for $N = 3, 10, 100$. Do the solutions obtained from Python verify our claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9ce6c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: -6.180339887498663\n",
      "            Iterations: 10\n",
      "            Function evaluations: 110\n",
      "            Gradient evaluations: 10\n",
      "Success: True | message: Optimization terminated successfully\n",
      "Perimeter = 6.180339887498663\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- Parameters --------\n",
    "N = 10  # Number of points\n",
    "\n",
    "# -------- Objective: polygon perimeter on the unit circle (edge length = 2*sin(u/2)) --------\n",
    "def perimeter(u):\n",
    "    u = np.asarray(u)\n",
    "    return np.sum(2.0 * np.sin(u / 2.0))\n",
    "\n",
    "def neg_perimeter(u):\n",
    "# SciPy minimizes; negate to perform maximization\n",
    "    return -perimeter(u)\n",
    "\n",
    "# -------- Constraints & initialization --------\n",
    "# Linear equality: sum(a) = 2π\n",
    "eq_cons = {'type': 'eq', 'fun': lambda u: np.sum(u) - 2.0 * np.pi}\n",
    "\n",
    "# Bounds: u_i ∈ [0, 2π] (upper bound helps numerics)\n",
    "bounds = [(0.0, 2.0 * np.pi)] * N\n",
    "\n",
    "# Initial guess: positive random vector normalized to 2π\n",
    "rng = np.random.default_rng(0)\n",
    "u0 = rng.random(N)\n",
    "u0 = u0 / u0.sum() * 2.0 * np.pi\n",
    "\n",
    "# -------- Solve (SLSQP) --------\n",
    "res = minimize(\n",
    "    neg_perimeter, u0,\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=[eq_cons],\n",
    "    options={'maxiter': 2000, 'ftol': 1e-12, 'disp': True}\n",
    ")\n",
    "\n",
    "uopt = res.x\n",
    "print(\"Success:\", res.success, \"| message:\", res.message)\n",
    "print(\"Perimeter =\", perimeter(uopt))\n",
    "\n",
    "# -------- Recover vertex angles x by cumulative sum (x[0]=0; others accumulate preceding gaps) --------\n",
    "x = np.zeros(N)\n",
    "x[1:] = np.cumsum(uopt[:-1])\n",
    "\n",
    "# -------- Plot --------\n",
    "fig, ax = plt.subplots()\n",
    "# Draw unit circle\n",
    "circle = plt.Circle((0, 0), 1.0, fill=False)\n",
    "ax.add_patch(circle)\n",
    "\n",
    "# Scatter vertices\n",
    "ax.scatter(np.cos(x), np.sin(x), s=40, label=\"points\")\n",
    "\n",
    "ax.set_aspect('equal', adjustable='box')\n",
    "ax.set_xlim(-1.1, 1.1)\n",
    "ax.set_ylim(-1.1, 1.1)\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(f\"N={N}, perimeter={perimeter(uopt):.6f}\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a181bad5",
   "metadata": {},
   "source": [
    "## 2. Convergence proof of Value iteration\n",
    "\n",
    "Let the Bellman optimality operator be\n",
    "$$\n",
    "(T^\\star V)(s)=\\max_a\\Big[\\,R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V(s')\\,\\Big],\n",
    "\\qquad \\gamma\\in[0,1).\n",
    "$$\n",
    "Let $V^\\star$ denote the optimal value function, i.e., $V^\\star=T^\\star V^\\star$.\n",
    "Value iteration is $V_{k+1}=T^\\star V_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553b1668",
   "metadata": {},
   "source": [
    "### 2.1 Contraction\n",
    "\n",
    "We first prove the operator is actually **contract**, i.e.\n",
    "$$\n",
    "||V_{k+1}-V^\\star|| \\leq ||V_k-V^\\star||\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n",
    "#### Step 1: A simple inequality about maxima\n",
    "For any real sequences $\\{x_a\\}$ and $\\{y_a\\}$,\n",
    "$$\n",
    "\\big|\\max_a x_a-\\max_a y_a\\big|\\;\\le\\;\\max_a |x_a-y_a|.\n",
    "$$\n",
    "*Proof sketch:* Let $a^\\star=\\arg\\max_a x_a$. Then\n",
    "$\\max_a x_a-\\max_a y_a \\le x_{a^\\star}-y_{a^\\star}\\le |x_{a^\\star}-y_{a^\\star}|\\le \\max_a|x_a-y_a|$,\n",
    "and the reverse direction is symmetric.\n",
    "\n",
    "#### Step 2: Subtract and bound one iteration\n",
    "Define the error $\\Delta_k:=V_k-V^\\star$. For any state $s$,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta_{k+1}(s)\n",
    "&=(T^\\star V_k)(s)-(T^\\star V^\\star)(s) \\\\\n",
    "&=\\max_a\\!\\left[R(s,a)+\\gamma\\!\\sum_{s'}P(s'|s,a)V_k(s')\\right]\n",
    "  -\\max_a\\!\\left[R(s,a)+\\gamma\\!\\sum_{s'}P(s'|s,a)V^\\star(s')\\right].\n",
    "\\end{aligned}\n",
    "$$\n",
    "Apply the inequality from Step 1 with\n",
    "$x_a=R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V_k(s')$ and\n",
    "$y_a=R(s,a)+\\gamma\\sum_{s'}P(s'|s,a)V^\\star(s')$:\n",
    "$$\n",
    "|\\Delta_{k+1}(s)|\n",
    "\\le\n",
    "\\gamma \\max_a \\left|\\sum_{s'}P(s'|s,a)\\big(V_k(s')-V^\\star(s')\\big)\\right|.\n",
    "$$\n",
    "Use $\\sum_{s'}P(s'|s,a)=1$ and the triangle inequality:\n",
    "$$\n",
    "|\\Delta_{k+1}(s)|\n",
    "\\le\n",
    "\\gamma \\max_a \\sum_{s'}P(s'|s,a)\\,|\\Delta_k(s')|\n",
    "\\le\n",
    "\\gamma \\|\\Delta_k\\|_\\infty.\n",
    "$$\n",
    "Taking supremum over $s$ gives\n",
    "$$\n",
    "\\|\\Delta_{k+1}\\|_\\infty \\le \\gamma \\|\\Delta_k\\|_\\infty.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b835d5",
   "metadata": {},
   "source": [
    "### 2.2 linear convergence\n",
    "Next we prove the convergence is actually **linear**, i.e.\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\gamma^k \\|V_0-V^\\star\\|_\\infty\n",
    "$$\n",
    "\n",
    "**(TODO) Answer:**\n",
    "\n",
    "By induction,\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty=\\|\\Delta_k\\|_\\infty\n",
    "\\leq \\gamma \\|\\Delta_{k-1}\\|_\\infty \\leq \\gamma^2 \\|\\Delta_{k-2}\\|_\\infty \\leq ... \\leq \\gamma^k \\|V_0-V^\\star\\|_\\infty \\xrightarrow[k\\to\\infty]{} 0.\n",
    "$$\n",
    "Hence $V_k$ converges to $V^\\star$ at a linear rate with factor $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd797a5",
   "metadata": {},
   "source": [
    "### 2.3 Practical stopping rule\n",
    "\n",
    "In practice we never know what is the true $V^\\star$. But what we can calculate is the difference between two iterations. Here we prove an error bound of $\\|V_{k}-V^\\star\\|_\\infty$ by $\\|V_{k+1} - V_k\\|_\\infty$:\n",
    "\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}\n",
    "$$\n",
    "\n",
    "and calculate what is the error tolerance of $\\|V_{k+1} - V_k\\|_\\infty$ we need, to have $\\|V_k-V^\\star\\|_\\infty \\leq 10^{-6}$, $\\gamma = 0.99$.\n",
    "\n",
    "**(TODO) Answer:**\n",
    "For any $V$,\n",
    "$$\n",
    "V-V^\\star=(V-T^\\star V)+(T^\\star V-T^\\star V^\\star).\n",
    "$$\n",
    "Taking $\\|\\cdot\\|_\\infty$ and using the bound from 2.1,\n",
    "$$\n",
    "\\|V-V^\\star\\|_\\infty\n",
    "\\le\n",
    "\\|V-T^\\star V\\|_\\infty+\\gamma\\|V-V^\\star\\|_\\infty\n",
    "\\;\\;\\Rightarrow\\;\\;\n",
    "\\|V-V^\\star\\|_\\infty \\le \\frac{\\|T^\\star V - V\\|_\\infty}{1-\\gamma}.\n",
    "$$\n",
    "take $V = V_k$ gives us\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\leq \\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma}\n",
    "$$\n",
    "This gives a practical stopping rule for value iteration.\n",
    "\n",
    "\n",
    "To have\n",
    "$$\n",
    "\\frac{\\|V_{k+1} - V_k\\|_\\infty}{1-\\gamma} \\leq 10^{-6}\n",
    "$$\n",
    "we can solve out\n",
    "$\\|V_{k+1} - V_k\\|_\\infty \\leq 10^{-8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a065e0a",
   "metadata": {},
   "source": [
    "### 2.4 The bound of iterations\n",
    "\n",
    "Assume $\\|V_1 - V_0\\|_\\infty = 1$, $\\gamma = 0.99$. How much iterations do we need to have $\\|V_k - V^\\star\\|_\\infty \\leq 10^{-6}$?\n",
    "\n",
    "**(TODO) Answer:**\n",
    "\n",
    "Using the previous conclusion\n",
    "$$\n",
    "\\|V_k-V^\\star\\|_\\infty \\le \\gamma^k\\|V_0-V^\\star\\|_\\infty,\n",
    "\\qquad\n",
    "\\|V_0-V^\\star\\|_\\infty \\le \\frac{1}{1-\\gamma}\\|V_{0}-V_1\\|_\\infty,\n",
    "$$\n",
    "we get\n",
    "$$\n",
    "\\|V_k - V^\\star\\|_\\infty \\le \\frac{\\gamma^k}{1-\\gamma}\\|V_1-V_0\\|_\\infty.\n",
    "$$\n",
    "\n",
    "Plug in $\\|V_1-V_0\\|_\\infty=1$ and $\\gamma=0.99$:\n",
    "$$\n",
    "\\|V_k - V^\\star\\|_\\infty \\le \\frac{0.99^k}{0.01}.\n",
    "$$\n",
    "We want $\\|V_k - V^\\star\\|_\\infty \\le 10^{-6}$, i.e.\n",
    "$$\n",
    "\\frac{0.99^k}{0.01} \\le 10^{-6}\n",
    "\\;\\Longleftrightarrow\\;\n",
    "0.99^k \\le 10^{-8}\n",
    "\\;\\Longleftrightarrow\\;\n",
    "k \\ge \\frac{\\ln(10^{-8})}{\\ln(0.99)} \\approx 1832.84.\n",
    "$$\n",
    "\n",
    "Then the smallest $k$ is $1833$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd169530",
   "metadata": {},
   "source": [
    "## 3. Cliffwalk\n",
    "\n",
    "Implement policy evaluation, policy improvement, value iteration, and policy iteration for the `CliffWalking` task. For clarity and reproducibility, We include a minimal reimplementation of the environment that mirrors Gymnasium’s dynamics and reward scheme.\n",
    "\n",
    "\n",
    "<figure style=\"text-align:center;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/ComputationalRobotics/2025-ES-AM-158-PSET/main/PSET1/cliffwalk.png\" width=\"600\" alt=\"Inscribed polygon\">\n",
    "  <figcaption style=\"color:#6a737d; font-style:italic;\">\n",
    "    Figure 3. Illustration to cliffwalk problem.\n",
    "  </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a9bc30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782a72c",
   "metadata": {},
   "source": [
    "**CliffWalking (Gym-compatible) — Specification**\n",
    "\n",
    "- **Grid:** 4 rows × 12 columns (row-major indexing; `state_id = row * 12 + col`; index origin at top-left in comments).\n",
    "- **Start:** bottom-left cell `(row=3, col=0)`.\n",
    "- **Goal:** bottom-right cell `(row=3, col=11)`.\n",
    "- **Actions (4):** up (0), right (1), down (2), left (3).\n",
    "- **Rewards:** −1 per step; −100 on entering a cliff cell; 0 at the goal.\n",
    "- **Termination:** episode ends upon reaching the goal; this states are terminal/absorbing. If reaching cliff will go back to start.\n",
    "\n",
    "**Transition table**\n",
    "\n",
    "- `P[state][action] → list[(prob, next_state, reward, done)]`\n",
    "- Deterministic dynamics: each list contains a single tuple with `prob = 1.0` after handling boundaries, cliff, and goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3422e418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State count=48, Action count=4\n"
     ]
    }
   ],
   "source": [
    "# Create Gym CliffWalking environment (v1).\n",
    "env_gym = gym.make(\"CliffWalking-v1\", render_mode=\"ansi\")\n",
    "nS, nA = env_gym.observation_space.n, env_gym.action_space.n\n",
    "# The CliffWalking grid is 4 × 12; actions are 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT.\n",
    "print(f\"State count={nS}, Action count={nA}\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a value function as a 2D grid (nrow × ncol).\n",
    "# Values can be any (nS,) array-like; states are indexed row-major:\n",
    "# State_id = row * ncol + col\n",
    "# -------------------------------------------------------------------\n",
    "def print_values(values, nrow: int, ncol: int, title: str = \"State Values\"):\n",
    "    \"\"\"Print a value table in grid form.\"\"\"\n",
    "    values = np.asarray(values).reshape(nrow, ncol)\n",
    "    print(title)\n",
    "    for r in range(nrow):\n",
    "        print(\" \".join(f\"{values[r, c]:6.2f}\" for c in range(ncol)))\n",
    "    print()\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Utility: pretty-print a policy on the CliffWalking grid.\n",
    "# \n",
    "# Accepted pi formats for each state s:\n",
    "# - Int a               : deterministic action\n",
    "# - Length-4 vector     : Q-values or preferences; we render argmax (ties shown)\n",
    "# - Length-4 probabilities (stochastic policy): greedy action(s) by max prob\n",
    "# \n",
    "# Notes:\n",
    "# - Uses Gym's action order: 0=UP(↑), 1=RIGHT(→), 2=DOWN(↓), 3=LEFT(←)\n",
    "# - Terminal states in CliffWalking (bottom row except col=0) are marked:\n",
    "# S at (last_row, 0), C for cliff cells (last_row, 1..ncol-2), G at (last_row, ncol-1)\n",
    "# -------------------------------------------------------------------\n",
    "def print_policy(pi, nrow: int, ncol: int, title: str = \"Policy\"):\n",
    "    \"\"\"Print a deterministic/stochastic policy.\n",
    "    - If pi is a list of lists (length 4): treat as stochastic over [up, down, left, right].\n",
    "    - We render the greedy direction; if ties exist, we list all best arrows.\n",
    "    \"\"\"\n",
    "    arrow = {0:\"^\", 1:\">\", 2:\"v\", 3:\"<\"}  # Order aligned with env actions in this notebook\n",
    "    print(title)\n",
    "    for i in range(nrow):\n",
    "        row_syms = []\n",
    "        for j in range(ncol):\n",
    "            s = i*ncol + j\n",
    "            p = pi[s]\n",
    "            # Determine best action(s)\n",
    "            if isinstance(p, list) and len(p) == 4:\n",
    "                best = np.argwhere(np.array(p) == np.max(p)).flatten().tolist()\n",
    "            elif isinstance(p, int):\n",
    "                best = [p]\n",
    "            else:\n",
    "                # Fallback: greedy over provided vector/array\n",
    "                arr = np.array(p, dtype=float).ravel()\n",
    "                best = np.argwhere(arr == np.max(arr)).flatten().tolist()\n",
    "            # Special case: terminals on bottom row except j==0\n",
    "            if i == nrow-1 and j > 0:\n",
    "                row_syms.append(\"T\")\n",
    "            else:\n",
    "                row_syms.append(\"\".join(arrow[a] for a in best))\n",
    "        print(\" \".join(sym if sym else \".\" for sym in row_syms))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f7f98",
   "metadata": {},
   "source": [
    "### 2.1 Define Environment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48012a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CliffWalkingEnv:\n",
    "    \"\"\"Cliff Walking environment (Gym-compatible dynamics)\n",
    "\n",
    "    State indexing\n",
    "    --------------\n",
    "    - Flattened row-major: state_id = row * ncol + col\n",
    "    - Rows: 0..nrow-1 (top → bottom), Cols: 0..ncol-1 (left → right)\n",
    "\n",
    "    Actions (match Gym/toy_text)\n",
    "    ----------------------------\n",
    "    - 0: UP (↑), 1: RIGHT (→), 2: DOWN (↓), 3: LEFT (←)\n",
    "\n",
    "    Grid (nrow=4, ncol=12)\n",
    "    ----------------------\n",
    "        [  0] [  1] [  2] [  3] [  4] [  5] [  6] [  7] [  8] [  9] [ 10] [ 11]\n",
    "        [ 12] [ 13] [ 14] [ 15] [ 16] [ 17] [ 18] [ 19] [ 20] [ 21] [ 22] [ 23]\n",
    "        [ 24] [ 25] [ 26] [ 27] [ 28] [ 29] [ 30] [ 31] [ 32] [ 33] [ 34] [ 35]\n",
    "        [36=S] [37=C] [38=C] [39=C] [40=C] [41=C] [42=C] [43=C] [44=C] [45=C] [46=C] [47=G]\n",
    "\n",
    "    Legend\n",
    "    ------\n",
    "    - S (start):  (row=3, col=0)   -> state 36\n",
    "    - C (cliff):  (row=3, col=1..10) -> states 37..46\n",
    "    - G (goal):   (row=3, col=11)  -> state 47\n",
    "\n",
    "    Termination & rewards\n",
    "    ---------------------\n",
    "    - Stepping into a cliff cell: reward = -100, done = False, go back to start\n",
    "    - Any other move:             reward = -1,   done = False\n",
    "    - Terminal states are absorbing: once in {goal}, any action keeps you there with reward 0.\n",
    "    \"\"\"\n",
    "\n",
    "    # Action constants for clarity\n",
    "    A_UP, A_RIGHT, A_DOWN, A_LEFT = 0, 1, 2, 3\n",
    "\n",
    "    def __init__(self, ncol: int = 12, nrow: int = 4):\n",
    "        self.ncol = int(ncol)\n",
    "        self.nrow = int(nrow)\n",
    "        self.nS = self.nrow * self.ncol\n",
    "        self.nA = 4\n",
    "        # Transition table: P[state][action] = [(prob, next_state, reward, done)]\n",
    "        self.P = self._create_P()\n",
    "\n",
    "    def _create_P(self):\n",
    "    # Allocate empty transition table\n",
    "        P = [[[] for _ in range(self.nA)] for _ in range(self.nS)]\n",
    "\n",
    "        # Movement deltas in (dx, dy), matching action order: 0↑, 1→, 2↓, 3←\n",
    "        # NOTE: x increases to the right (columns), y increases downward (rows).\n",
    "        deltas = {\n",
    "            self.A_UP:    ( 0, -1),\n",
    "            self.A_RIGHT: ( 1,  0),  # (1, 0) Written to hint order; same as (1, 0)\n",
    "            self.A_DOWN:  ( 0,  1),\n",
    "            self.A_LEFT:  (-1,  0),\n",
    "        }\n",
    "\n",
    "        start_s = (self.nrow - 1) * self.ncol + 0\n",
    "        goal_s  = (self.nrow - 1) * self.ncol + (self.ncol - 1)\n",
    "\n",
    "        for r in range(self.nrow):\n",
    "            for c in range(self.ncol):\n",
    "                s = r * self.ncol + c\n",
    "\n",
    "                if r == self.nrow - 1 and c > 0:\n",
    "                    for a in range(self.nA):\n",
    "                        P[s][a] = [(1.0, s, 0.0, True)]\n",
    "                    continue\n",
    "\n",
    "                for a in range(self.nA):\n",
    "                    dx, dy = deltas[a]\n",
    "\n",
    "                    nc = min(self.ncol - 1, max(0, c + dx))\n",
    "                    nr = min(self.nrow - 1, max(0, r + dy))\n",
    "\n",
    "                    ns = nr * self.ncol + nc\n",
    "                    reward = -1.0\n",
    "                    done = False\n",
    "\n",
    "                    if nr == self.nrow - 1 and 1 <= nc <= self.ncol - 2:\n",
    "                        ns = start_s          \n",
    "                        reward = -100.0\n",
    "                        done = False\n",
    "\n",
    "                    elif nr == self.nrow - 1 and nc == self.ncol - 1:\n",
    "                        done = True\n",
    "\n",
    "                    P[s][a] = [(1.0, ns, reward, done)]\n",
    "\n",
    "        return P\n",
    "\n",
    "\n",
    "# Build environment\n",
    "env = CliffWalkingEnv(ncol=12, nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572fedfe",
   "metadata": {},
   "source": [
    "### 2.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72beca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 494 iteration(s).\n",
      "Value Function under Random Policy\n",
      "-143.21 -147.36 -151.35 -153.93 -155.11 -155.06 -153.67 -150.46 -144.47 -134.46 -119.99 -105.22\n",
      "-164.99 -174.34 -180.41 -183.52 -184.80 -184.83 -183.63 -180.70 -174.70 -163.02 -141.34 -108.39\n",
      "-207.96 -237.09 -246.20 -249.36 -250.43 -250.52 -249.79 -247.82 -243.20 -231.68 -199.50 -97.21\n",
      "-261.35   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(env, pi, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Iterative policy evaluation for a given stochastic policy π(a|s).\n",
    "\n",
    "    Args:\n",
    "        env: environment with a tabular transition model env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  policy probabilities shaped [nS][4]; each pi[s] is a length-4 list\n",
    "             over actions [UP, RIGHT, DOWN, LEFT].\n",
    "        gamma: discount factor ∈ [0, 1).\n",
    "        theta: convergence threshold on the ∞-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        v: list of state values of length nS.\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    v = [0.0] * nS  # Initialize V(s)=0\n",
    "    it = 1  # Iteration counter (logging only)\n",
    "\n",
    "    while True:\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            v_sum = 0.0  # Σ_a π(a|s) * Q(s,a)\n",
    "            for a in range(4):\n",
    "                q = 0.0\n",
    "                # Sum over outcomes (deterministic here, but we keep the general form)\n",
    "                for (p, next_state, r, done) in env.P[s][a]:\n",
    "                    # If terminal, no bootstrap term from V(next_state)\n",
    "                    q += p * (r + gamma * v[next_state])\n",
    "                # Weight by policy π(a|s)\n",
    "                v_sum += pi[s][a] * q\n",
    "\n",
    "            new_v[s] = v_sum\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "        it += 1\n",
    "\n",
    "    print(f\"Policy evaluation converged in {it} iteration(s).\")\n",
    "    return v\n",
    "\n",
    "\n",
    "# --- Example: evaluate a uniform random policy ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "gamma = 0.95\n",
    "\n",
    "v = policy_evaluation(env, pi, gamma)\n",
    "\n",
    "# Pretty-print the value function as a 4×12 grid\n",
    "print_values(v, env.nrow, env.ncol, title=\"Value Function under Random Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da66ff59",
   "metadata": {},
   "source": [
    "### 2.3 Policy Iteration\n",
    "\n",
    "Policy Iteration alternates between:\n",
    "1) **Policy Evaluation**: compute the state-value function $V^{\\pi}$ of the current policy $\\pi$\n",
    "2) **Policy Improvement**: update $\\pi$ to be greedy w.r.t. $V^{\\pi}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37e91266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy evaluation converged in 494 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 450 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 403 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 15 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy evaluation converged in 15 iteration(s).\n",
      "Policy improvement completed.\n",
      "Policy iteration converged in 5 improvement step(s).\n",
      "Optimal Value Function\n",
      "-10.25  -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85\n",
      " -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95\n",
      " -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95  -1.00\n",
      " -9.73   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n",
      "Optimal Policy\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      "> > > > > > > > > > > v\n",
      "^ T T T T T T T T T T T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def policy_improvement(env, pi, v, gamma=0.95):\n",
    "    \"\"\"Greedy policy improvement w.r.t. the current state-value function V.\n",
    "\n",
    "    For each state s:\n",
    "      1) Compute Q(s,a) = Σ_{s'} P(s'|s,a)[ r(s,a,s') + γ V(s') ] for all a.\n",
    "      2) Find the action(s) with maximal Q(s,a).\n",
    "      3) Update π(·|s) to split probability uniformly among all maximizers (tie-aware).\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with transitions env.P where\n",
    "             P[s][a] = [(prob, next_state, reward, done)].\n",
    "        pi:  Current (possibly stochastic) policy, shape [nS][4]; updated in-place.\n",
    "        v:   Current state-value function V(s), length nS.\n",
    "        gamma: Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        pi: The improved policy (same object, updated in-place).\n",
    "    \"\"\"\n",
    "    nS = env.nrow * env.ncol\n",
    "    nA = 4\n",
    "    eps = 1e-8  # Numerical tolerance for tie-breaking\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for (p, next_state, r, done) in env.P[s][a]:\n",
    "                # If terminal, no bootstrap term from V(next_state)\n",
    "                q += p * (r + gamma * v[next_state])\n",
    "            q_list.append(q)\n",
    "\n",
    "        q = np.array(q_list, dtype=float)\n",
    "        max_q = q.max()\n",
    "        # Tie-aware argmax using tolerance\n",
    "        opt_u = np.isclose(q, max_q, rtol=0.0, atol=eps)\n",
    "        k = int(opt_u.sum())\n",
    "        pi[s] = (opt_u / k).astype(float).tolist()\n",
    "\n",
    "    print(\"Policy improvement completed.\")\n",
    "    return pi\n",
    "\n",
    "\n",
    "# --- Policy Iteration loop ---\n",
    "pi = [[0.25, 0.25, 0.25, 0.25] for _ in range(env.nrow * env.ncol)]\n",
    "iters = 0\n",
    "while True:\n",
    "    v = policy_evaluation(env, pi, gamma)  # Assumes policy_evaluation is defined\n",
    "    old_pi = copy.deepcopy(pi)\n",
    "    new_pi = policy_improvement(env, pi, v, gamma)\n",
    "    iters += 1\n",
    "    if old_pi == new_pi:  # Policy is stable\n",
    "        print(f\"Policy iteration converged in {iters} improvement step(s).\")\n",
    "        break\n",
    "\n",
    "# Report results\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b7f8d",
   "metadata": {},
   "source": [
    "### 2.4 Value Iteration\n",
    "\n",
    "Value Iteration applies **Bellman optimality** updates directly to $V$. Or one can treat value iteration as one step policy evaluation plus one step policy improvement.\n",
    "\n",
    "After convergence, extract the greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4065588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "Optimal Value Function (Value Iteration)\n",
      "-10.25  -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85\n",
      " -9.73  -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95\n",
      " -9.19  -8.62  -8.03  -7.40  -6.73  -6.03  -5.30  -4.52  -3.71  -2.85  -1.95  -1.00\n",
      " -9.73   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
      "\n",
      "Optimal Policy (Value Iteration)\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      ">v >v >v >v >v >v >v >v >v >v >v v\n",
      "> > > > > > > > > > > v\n",
      "^ T T T T T T T T T T T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def iterate(env, gamma=0.95, theta=1e-10):\n",
    "    \"\"\"Value Iteration.\n",
    "\n",
    "    Updates V(s) <- max_a Σ_{s'} P(s'|s,a) [ r(s,a,s') + γ V(s') ]\n",
    "    until the maximum state-wise change is below `theta`.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment exposing env.P with\n",
    "             P[s][a] = [(prob, next_state, reward, done)] and grid sizes nrow, ncol.\n",
    "        gamma (float): Discount factor in [0, 1).\n",
    "        theta (float): Convergence threshold on the infinity-norm of value updates.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: The converged state-value function V of length nS (= nrow * ncol).\n",
    "\n",
    "    Notes:\n",
    "        - Terminal states are modeled as absorbing with reward 0 in `env.P`.\n",
    "          The Bellman backup naturally yields V(terminal) = 0.\n",
    "        - `deltas` (max per-iteration change) is tracked for debugging but not returned.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    deltas = []\n",
    "    iters = 0\n",
    "    v = [0.0] * nS\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        max_diff = 0.0\n",
    "        new_v = [0.0] * nS\n",
    "\n",
    "        for s in range(nS):\n",
    "            # Bellman optimality backup: V(s) = max_a Q(s,a)\n",
    "            q_list = []\n",
    "            for a in range(nA):\n",
    "                q = 0.0\n",
    "                for (p, next_state, r, done) in env.P[s][a]:\n",
    "                    q += p * (r if done else r + gamma * v[next_state])\n",
    "                q_list.append(q)\n",
    "            new_v[s] = max(q_list)\n",
    "            max_diff = max(max_diff, abs(new_v[s] - v[s]))\n",
    "\n",
    "        v = new_v\n",
    "        deltas.append(max_diff)\n",
    "        if max_diff < theta:\n",
    "            break\n",
    "    print(iters)\n",
    "    return v\n",
    "\n",
    "\n",
    "def greedy_policy(env, v, gamma=0.95):\n",
    "    \"\"\"Extract a greedy (tie-aware) policy from a value function.\n",
    "\n",
    "    For each state s, compute Q(s,a) and set π(a|s)=1/k for all actions a that\n",
    "    achieve the maximal Q-value (ties split uniformly); 0 otherwise.\n",
    "\n",
    "    Args:\n",
    "        env: Tabular environment with env.P.\n",
    "        v (list[float]): State-value function V(s).\n",
    "        gamma (float): Discount factor.\n",
    "\n",
    "    Returns:\n",
    "        list[list[float]]: Policy π of shape [nS][4], each row summing to 1.\n",
    "    \"\"\"\n",
    "    nS, nA = env.nrow * env.ncol, 4\n",
    "    pi = [[0.0] * nA for _ in range(nS)]\n",
    "    eps = 1e-8  # Numerical tolerance for tie detection\n",
    "\n",
    "    for s in range(nS):\n",
    "        q_list = []\n",
    "        for a in range(nA):\n",
    "            q = 0.0\n",
    "            for (p, next_state, r, done) in env.P[s][a]:\n",
    "                q += p * (r if done else r + gamma * v[next_state])\n",
    "            q_list.append(q)\n",
    "\n",
    "        q_list = np.array(q_list, dtype=float)\n",
    "        max_q = q_list.max()\n",
    "        # Tie-aware argmax\n",
    "        opt_u = np.isclose(q_list, max_q, rtol=0.0, atol=eps)\n",
    "        k = int(opt_u.sum())\n",
    "        pi[s] = (opt_u / k).astype(float).tolist()\n",
    "\n",
    "    return pi\n",
    "\n",
    "\n",
    "# ----- Run Value Iteration and extract greedy policy -----\n",
    "gamma = 0.95  # Discount factor\n",
    "v = iterate(env, gamma=gamma)  # Assumes `env` is already constructed\n",
    "pi = greedy_policy(env, v, gamma=gamma)\n",
    "\n",
    "# Pretty-print results (assumes `print_values` and `print_policy` are defined)\n",
    "print_values(v, env.nrow, env.ncol, title=\"Optimal Value Function (Value Iteration)\")\n",
    "print_policy(pi, env.nrow, env.ncol, title=\"Optimal Policy (Value Iteration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e865218",
   "metadata": {},
   "source": [
    "## 3. Matrix–vector Representation of DP\n",
    "\n",
    "We’ll use a small finite MDP ($|X|=16$, $|A|=4$) and **matrix** forms to compute the optimal $Q$-function:\n",
    "\n",
    "3.1. Build the **transition matrix**\n",
    "   $\n",
    "   P \\in \\mathbb{R}^{|X||A|\\times |X|}\n",
    "   $\n",
    "   and the **immediate reward** vector\n",
    "   $\n",
    "   r \\in \\mathbb{R}^{|X||A|}.\n",
    "   $\n",
    "\n",
    "3.2. Using the matrix form of the $Q$-value function $Q_\\pi$ and the value function $V_\\pi$ to write down the bellman equation.\n",
    "\n",
    "3.3. Define the **Bellman optimality operator**:\n",
    "   $$\n",
    "   T^\\star(Q) = r + \\gamma\\, P\\, J_Q,\n",
    "   $$\n",
    "   where\n",
    "   $$\n",
    "   (J_Q)(x) = \\max_{a} Q(x,a).\n",
    "   $$\n",
    "   Iterating $Q_{k+1} = T^\\star(Q_k)$ converges to the optimal $Q^\\star$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35440cec",
   "metadata": {},
   "source": [
    "### 4×4 Gridworld — From Bottom‑Left (Start) to Top‑Right (Goal)\n",
    "\n",
    "**States:** 16 cells in a 4×4 grid, row-major indexing with top-left as (row=0, col=0).\n",
    "State id: `s = row * 4 + col`, rows increase downward.\n",
    "\n",
    "**Start:** bottom-left `(row=3, col=0)` → `s_start = 12`\n",
    "**Goal:** top-right `(row=0, col=3)` → `s_goal = 3`\n",
    "\n",
    "**Actions (4):**\n",
    "- `a=0` → UP (↑)\n",
    "- `a=1` → RIGHT (→)\n",
    "- `a=2` → DOWN (↓)\n",
    "- `a=3` → LEFT (←)\n",
    "\n",
    "**Dynamics:** Deterministic. If an action would leave the grid world, the agent stays in place.\n",
    "\n",
    "**Rewards (maximize):**\n",
    "- `-1` per step\n",
    "- `0` in the goal\n",
    "\n",
    "**Terminal:** The goal is absorbing (from goal, any action keeps you at goal with reward 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76ef97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\n",
      " 0  1  2  G \n",
      " 4  5  6  7\n",
      " 8  9 10 11\n",
      " S  13 14 15\n"
     ]
    }
   ],
   "source": [
    "# Grid size\n",
    "nrow, ncol = 4, 4\n",
    "nS = nrow * ncol  # |X| = 16\n",
    "nA = 4  # |A| = 4 (UP, RIGHT, DOWN, LEFT)\n",
    "\n",
    "# Start (bottom-left) and Goal (top-right)\n",
    "s_start = (nrow - 1) * ncol + 0  # 12\n",
    "s_goal  = 0 * ncol + (ncol - 1)  # 3\n",
    "\n",
    "# Row-major state id\n",
    "def s_id(r, c):\n",
    "    return r * ncol + c\n",
    "\n",
    "# For state-action row index in matrices of shape (nS*nA, ...)\n",
    "def sa_id(s, a):\n",
    "    return s * nA + a\n",
    "\n",
    "# Action deltas: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "DELTAS = {\n",
    "    0: (-1,  0),  # UP:    row-1\n",
    "    1: ( 0,  1),  # RIGHT: col+1\n",
    "    2: ( 1,  0),  # DOWN:  row+1\n",
    "    3: ( 0, -1),  # LEFT:  col-1\n",
    "}\n",
    "\n",
    "# Quick sanity checks and a tiny ASCII map\n",
    "print(\"Grid 4×4. Start=S (row=3,col=0), Goal=G (row=0,col=3)\")\n",
    "for rrow in range(nrow):\n",
    "    line = []\n",
    "    for ccol in range(ncol):\n",
    "        s = s_id(rrow, ccol)\n",
    "        if s == s_start:\n",
    "            line.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            line.append(\" G \")\n",
    "        else:\n",
    "            line.append(f\"{s:2d}\")\n",
    "    print(\" \".join(line))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2d4d6",
   "metadata": {},
   "source": [
    "### 3.1 Build Transition Matrix and Reward Vector\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- **Transition matrix** $P \\in \\mathbb{R}^{|X||A|\\times |X|}$\n",
    "  Rows index state–action pairs $(x,a)$, columns index next states $x'$.\n",
    "  Entry:\n",
    "  $$\n",
    "  P[(x,a),\\,x'] \\;\\equiv\\; \\Pr\\{X_{t+1}=x' \\mid X_t=x,\\; A_t=a\\}.\n",
    "  $$\n",
    "  Row-wise normalization holds: $\\sum_{x'} P[(x,a),x'] = 1$ for every $(x,a)$.\n",
    "\n",
    "- **Reward vector** $r \\in \\mathbb{R}^{|X||A|}$ (reward maximization form)\n",
    "  Each entry is the one-step expected reward under $(x,a)$:\n",
    "  $$\n",
    "  r[(x,a)] \\;\\equiv\\; \\mathbb{E}\\big[\\,R_{t+1}\\mid X_t=x,\\; A_t=a\\,\\big].\n",
    "  $$\n",
    "\n",
    "**Indexing note.**\n",
    "A convenient index for $(x,a)$ is\n",
    "$$\n",
    "i = x\\,|A| + a\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86995a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build P (|X||A| × |X|) and r (|X||A|)\n",
    "P = np.zeros((nS * nA, nS), dtype=float)\n",
    "r = np.zeros(nS * nA, dtype=float)\n",
    "\n",
    "for s in range(nS):\n",
    "# This will give // and %\n",
    "    r0, c0 = divmod(s, ncol)\n",
    "\n",
    "    for a in range(nA):\n",
    "        _s_id = sa_id(s, a)\n",
    "\n",
    "        # Goal is absorbing with reward 0\n",
    "        if s == s_goal:\n",
    "            P[_s_id, s_goal] = 1.0\n",
    "            r[_s_id] = 0.0\n",
    "            continue\n",
    "\n",
    "        dr, dc = DELTAS[a]\n",
    "        rr = min(nrow - 1, max(0, r0 + dr))\n",
    "        cc = min(ncol - 1, max(0, c0 + dc))\n",
    "        s_next = s_id(rr, cc)\n",
    "\n",
    "        # Deterministic transition\n",
    "        P[_s_id, s_next] = 1.0\n",
    "\n",
    "        # Reward: -1 per step, 0 in goal (already handled above)\n",
    "        r[_s_id] = -1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5a23e0",
   "metadata": {},
   "source": [
    "### 3.2 Matrix Form of Bellman Consistency and Bellman equation\n",
    "\n",
    "Q-evaluation when a fixed policy $\\pi$ is given:\n",
    "\n",
    "$$\n",
    "Q_\\pi(x,a) = r(x,a) + \\gamma \\,\\mathbb{E}_{x'\\sim P(\\cdot \\mid x,a)} \\, V_\\pi(x') \\tag{3.3(1)}\n",
    "$$\n",
    "\n",
    "The bellman equation:\n",
    "$$\n",
    "Q^\\star(x,a) \\;=\\; r(x,a) \\;+\\; \\gamma \\, \\mathbb{E}_{x' \\sim P(\\cdot \\mid x,a)}\n",
    "\\left\\{ \\max_{a' \\in A} Q^\\star(x',a') \\right\\},\n",
    "\\qquad \\forall (x,a) \\in X \\times A. \\tag{3.3(2)}\n",
    "$$\n",
    "\n",
    "where $Q^\\star$ is the optimal $Q$-value function. Similarly, let us define\n",
    "\n",
    "$$\n",
    "J_Q(x) = \\max_{a \\in A} Q(x,a).\n",
    "$$\n",
    "\n",
    "Question: How to write these equations (3.3(1))&(2) in matrix and operator form?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570aac90",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "$$\n",
    "Q_\\pi = r + \\gamma PV_\\pi\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^\\star \\;=\\; r + \\gamma P J_{Q^\\star},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054a258d",
   "metadata": {},
   "source": [
    "### 3.3 Solve bellman equation.\n",
    "\n",
    "Note that $J_Q$ has dimension $|X|$. With these notations, the *Bellman optimality operator* is defined as\n",
    "\n",
    "$$\n",
    "T^\\star Q \\;=\\; g + \\gamma P J_Q,\n",
    "\\tag{2.27}\n",
    "$$\n",
    "\n",
    "which is nothing but a matrix representation of the right-hand side of Bellman equation.\n",
    "This allows us to concisely write the Bellman equation as\n",
    "\n",
    "$$\n",
    "Q = T^\\star Q.\n",
    "\\tag{2.28}\n",
    "$$\n",
    "\n",
    "One can do to solve this equation is through *fix-point iteration*:\n",
    "$$\n",
    "Q_{n+1} = T^\\star Q_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b44ffe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 8 iterations.\n",
      "\n",
      "Optimal state values J_Q (V*) on the grid:\n",
      " -2.85  -1.95  -1.00  G \n",
      " -3.71  -2.85  -1.95  -1.00\n",
      " -4.52  -3.71  -2.85  -1.95\n",
      " S   -4.52  -3.71  -2.85\n"
     ]
    }
   ],
   "source": [
    "Q = np.zeros((nS * nA), dtype=float)\n",
    "for i in range(1000):\n",
    "    old_Q = Q.copy()\n",
    "    J_Q = Q.reshape(nS, nA).max(axis=1)  # V(s) = max_a Q(s,a)\n",
    "    Q = r + gamma * P.dot(J_Q)\n",
    "    if np.max(np.abs(Q - old_Q)) < 1e-10:\n",
    "        print(f\"Converged in {i+1} iterations.\")\n",
    "        break\n",
    "J_Q = Q.reshape(nS, nA).max(axis=1)    \n",
    "print(\"\\nOptimal state values J_Q (V*) on the grid:\")\n",
    "for r0 in range(nrow):\n",
    "    row_vals = []\n",
    "    for c0 in range(ncol):\n",
    "        s = r0 * ncol + c0\n",
    "        if s == s_start:\n",
    "            row_vals.append(\" S \")\n",
    "        elif s == s_goal:\n",
    "            row_vals.append(\" G \")\n",
    "        else:\n",
    "            row_vals.append(f\"{J_Q[s]:6.2f}\")\n",
    "    print(\" \".join(row_vals))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl-pset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
