{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem set 3: DQN, Policy gradient and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Due: 11:59pm, November 2, 2025**\n",
    "\n",
    "In the last PSET of reinforcement learning, you will implement the DQN algorithm together with vanilla policy gradient (REINFORCE) and its variants. In the last problem you will train a half-cheetah with stable baseline 3 package. \n",
    "\n",
    "- Problem 1 requires implementing DQN and double DQN. **TODOs:**\n",
    "  \n",
    "  - 1.1 Finish vanilla DQN (20 pt)\n",
    "  - 1.2 Finish double DQN (5 pt)\n",
    "\n",
    "- Problem 2 verify the policy gradient theory and requires implementing REINFORCE with learned value function. **TODOs:**\n",
    "  \n",
    "  - 2.1 Verify different policy gradient estimator (15 pt)\n",
    "  - 2.2 Implement REINFORCE with learned baseline (10 pt)\n",
    "\n",
    "- Problem 3 requires implementing vanilla on-policy actor-critic algorithm. **TODOs:**\n",
    "  - 3.1 Implement vanilla actor-critic (25 pt)\n",
    "\n",
    "All of these three algorithms works on `gym`'s `Acrobot-v1` environments.\n",
    "\n",
    "- Problem 4 requires implementing PPO algorithm. **TODOs:**\n",
    "  - 4.1 Implement PPO-cliping (25 pt)\n",
    "\n",
    "- Problem 5 **(Bonus)** help you try stable baseline 3 on `gym`'s `Half-cheetah-v4` environment. **TODOs:**\n",
    "  - 5.1 Tune the parameter in stable baseline 3 (**Note the training can take 15 min**) (20 pt)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will implement DQN on `Acrobot-v1` environment using `gym`.\n",
    "\n",
    "### Algorithm recap\n",
    "\n",
    "1. **Q-network**\n",
    "   - Function approximator $Q_\\theta(s,a)$ for action values (here: a small MLP).\n",
    "\n",
    "2. **Target network**\n",
    "   - A copy of the online network with parameters $\\theta^-$ that are updates periodically to stabilize training.\n",
    "\n",
    "3. **Experience replay**\n",
    "   - A replay buffer of transitions $(s,a,r,s',\\text{done})$. Sample i.i.d. minibatches to break temporal correlations.\n",
    "\n",
    "4. **Behavior policy**\n",
    "   - $\\epsilon$-greedy: with probability $\\epsilon$ choose a random action; otherwise choose $ \\arg\\max_a Q_\\theta(s,a) $.\n",
    "\n",
    "5. **TD targets**\n",
    "   - **Standard DQN**:\n",
    "     $$\n",
    "     y = r + \\gamma \\max_{a'} Q_{\\theta^-}(s', a')\n",
    "     $$\n",
    "   - **Double DQN**:\n",
    "     $$\n",
    "     a^\\star = \\arg\\max_{a'} Q_{\\theta}(s', a'), \\quad\n",
    "     y = r + \\gamma \\, Q_{\\theta^-}(s', a^\\star)\n",
    "     $$\n",
    "     “Online net selects, target net evaluates” reduces overestimation. In comparison to Double Q-learning, the weights of the second network $\\theta$ are replaced with the weights of the target network $\\theta^−$ for the evaluation of the current greedy policy. The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network.\n",
    "\n",
    "6. **Loss & optimization**\n",
    "   - Regress $Q_\\theta(s,a)$ to target $y$ using MSE loss; backpropagate to update $\\theta$.\n",
    "\n",
    "### Environment & action space\n",
    "\n",
    "- **Env**: `Acrobot-v1` (double pendulum swing-up) [Link](https://gymnasium.farama.org/environments/classic_control/acrobot/)\n",
    "- **Observation**: 6D —- $\\cos\\theta_1, \\sin\\theta_1, \\cos\\theta_2, \\sin\\theta_2, \\dot\\theta_1, \\dot\\theta_2$  \n",
    "- **Actions**: Discrete 3 actions —- torques $-1, 0, +1$  \n",
    "- **Reward**: $-1$ per step until the goal is reached (or the episode times out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Implement DQN with gym\n",
    "\n",
    "**TODO:** Fill in the three TODO blocks.\n",
    "- implement a simple MLP\n",
    "- implement the replaybuffer class\n",
    "- implement the main algorithm\n",
    "\n",
    "All the given code is for reference. If you find it inconvenient feel free to write yourself.\n",
    "\n",
    "Note the final average return should be around $-100$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "# ------------------- config  -------------------\n",
    "ENV_ID                 = \"Acrobot-v1\"\n",
    "SEED                   = 42\n",
    "GAMMA                  = 0.995\n",
    "LR                     = 1e-3\n",
    "BATCH_SIZE             = 64\n",
    "BUFFER_SIZE            = 100_000\n",
    "START_TRAINING_AFTER   = 1000       # warmup steps\n",
    "TARGET_UPDATE_FREQ     = 10       # steps (hard update)\n",
    "MAX_EPISODES           = 800\n",
    "\n",
    "GRAD_CLIP_NORM         = 10.0\n",
    "PRINT_EVERY_EPISODES   = 10\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "state_dims = int(np.prod(env.observation_space.shape))  # Acrobot: 6\n",
    "num_actions = env.action_space.n                        # Acrobot: 3\n",
    "print(f\"[Env] {ENV_ID} | obs_dim={state_dims}, n_actions={num_actions}\")\n",
    "\n",
    "# ------------------- gym environment -> tensor -------------------\n",
    "class TWrapper(gym.Wrapper):\n",
    "    def __init__(self, env): super().__init__(env)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        return torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0), info\n",
    "    def step(self, action):\n",
    "        a = int(action.item()) if isinstance(action, torch.Tensor) else int(action)\n",
    "        obs, r, term, trunc, info = self.env.step(a)\n",
    "        done = bool(term or trunc)\n",
    "        obs_t = torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0)\n",
    "        r_t   = torch.tensor([[r]], dtype=torch.float32)\n",
    "        d_t   = torch.tensor([[done]], dtype=torch.bool)\n",
    "        return obs_t, r_t, d_t, info\n",
    "\n",
    "env = TWrapper(env)\n",
    "\n",
    "# ------------------- Q network -------------------\n",
    "class QNetwork(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 1.1: Implement a simple MLP\n",
    "    #########################################\n",
    "   \n",
    "\n",
    "q_net = QNetwork(state_dims, num_actions)\n",
    "tgt_net = copy.deepcopy(q_net).eval()\n",
    "\n",
    "# ------------------- simple replay buffer -------------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=BUFFER_SIZE):\n",
    "        self.capacity, self.mem, self.pos = capacity, [], 0\n",
    "    #########################################\n",
    "    #TODO 1.1: Implement a ReplayBuffer\n",
    "    # capacity: max number of transitions to store\n",
    "    # mem: list of transitions\n",
    "    # pos: next position to insert\n",
    "    # push: add a transition\n",
    "    # sample: random sample a batch of transitions\n",
    "    #########################################\n",
    "    \n",
    "buffer = ReplayBuffer()\n",
    "optim  = Adam(q_net.parameters(), lr=LR)\n",
    "\n",
    "# ------------------- greedy / epsilon-greedy -------------------\n",
    "@torch.no_grad()\n",
    "def act_epsilon_greedy(state: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "    if torch.rand(1).item() < eps:\n",
    "        return torch.randint(num_actions, (1, 1))\n",
    "    q = q_net(state)\n",
    "    return torch.argmax(q, dim=-1, keepdim=True)\n",
    "\n",
    "# ------------------- train loop (Double DQN target) -------------------\n",
    "def train():\n",
    "    returns_hist, loss_hist = [], []\n",
    "\n",
    "    for ep in range(1, MAX_EPISODES + 1):\n",
    "        #########################################\n",
    "        #TODO 1.1: Implement the main algorithm here\n",
    "        #########################################\n",
    "        #########################################\n",
    "        #TODO 1.2: Change from DQN to Double DQN\n",
    "        #########################################\n",
    "        \n",
    "    plot_stats({\"Returns\": returns_hist, \"Loss\": loss_hist})\n",
    "\n",
    "# ------------------- plotting -------------------\n",
    "def _smooth(x, w=21):\n",
    "    if len(x) < w: return x\n",
    "    k = w // 2\n",
    "    return [np.mean(x[max(0, i-k):min(len(x), i+k+1)]) for i in range(len(x))]\n",
    "\n",
    "def plot_stats(stats: dict, win: int = 21):\n",
    "    fig, axs = plt.subplots(len(stats), 1, figsize=(9, 5), tight_layout=True)\n",
    "    if len(stats) == 1: axs = [axs]\n",
    "    for ax, (k, v) in zip(axs, stats.items()):\n",
    "        ax.plot(_smooth(v, win))\n",
    "        ax.set_title(k)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "train()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Change classical DQN to double DQN\n",
    "\n",
    "Use **two networks**:\n",
    "- **Online** network selects the next action  \n",
    "  $$\n",
    "  a^* = \\arg\\max_{a'} Q_{\\text{online}}(s', a').\n",
    "  $$\n",
    "- **Target** network evaluates that action  \n",
    "  $$\n",
    "  y_{\\text{DDQN}} = r + \\gamma\\, Q_{\\text{target}}(s', a^*).\n",
    "  $$\n",
    "  \n",
    "This decoupling reduces overestimation while keeping the update otherwise unchanged.\n",
    "\n",
    "In the code you will only need to change several lines. \n",
    "\n",
    "**TODO:** Comment the vanilla DQN and write Double DQN at the same place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- #### reparameterization trick:\n",
    "\n",
    "Let $x$ be a random variable whose distribution depends on $\\theta$. Write this as $x\\sim p_\\theta(x)$. For any measurable $V:\\mathcal X\\to\\mathbb R$,\n",
    "$$\n",
    "\\mathbb{E}\\big[V(f(\\theta))\\big] \\;=\\; \\mathbb{E}_{x\\sim p_\\theta}[V(x)],\n",
    "$$\n",
    "where $p_\\theta$ is the distribution of $x$ induced by $\\theta$. This help us move complex functions from expectation to distribution.\n",
    "\n",
    "Take the gradient gives us\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbb{E}_{x\\sim p_\\theta}[V(x)] = \\int \\nabla p_\\theta(x) V(x) dx = \\int \\nabla \\ln(p_\\theta(x)) V(x) p_\\theta(x)dx = \\mathbb{E}_{x\\sim p_\\theta}[V(x) \\nabla \\ln(p_\\theta(x))]\n",
    "$$\n",
    "\n",
    "So now we only need to take derivative of the distribution.\n",
    "\n",
    "### RL as an Expectation over Trajectories\n",
    "The cost over a distribution of $s_0$ gives the definition of reward function $J$:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{s_0\\sim\\rho}[V^{\\pi_\\theta}(s_0)]\n",
    "$$\n",
    "\n",
    "But use the trick above we can rewrite that using trajectory. A trajectory is\n",
    "$$\n",
    "\\tau=(s_0,a_0,s_1,a_1,\\ldots,s_T),\n",
    "$$\n",
    "generated by initial state distribution $\\rho$, policy $\\pi_\\theta(a\\mid s)$, and dynamics $P(s'\\mid s,a)$. The trajectory distribution is\n",
    "$$\n",
    "p_\\theta(\\tau)=\\rho(s_0)\\prod_{t=0}^{T-1}\\pi_\\theta(a_t\\mid s_t)\\,P(s_{t+1}\\mid s_t,a_t).\n",
    "$$\n",
    "Define the discounted return\n",
    "$$\n",
    "G(\\tau)=\\sum_{t=0}^{T-1}\\gamma^t\\,r(s_t,a_t).\n",
    "$$\n",
    "Then the performance objective is\n",
    "$$\n",
    "\\,J(\\theta)=\\mathbb{E}_{\\tau\\sim p_\\theta}[G(\\tau)]\\,\n",
    "$$\n",
    "\n",
    "This helps move the $\\theta$ to distribution.\n",
    " -->\n",
    "\n",
    "### Recall: Policy-Gradient Theorem\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\nabla_\\theta \\mathbb{E}_{\\tau\\sim p_\\theta}[G(\\tau)]\n",
    "= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[G(\\tau)\\,\\nabla_\\theta\\log p_\\theta(\\tau)\\right] \\\\\n",
    "&= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G(\\tau)\\right] \\\\\n",
    "\\end{aligned} \\tag{F1}\n",
    "$$\n",
    "\n",
    "This is the first gradient formulation we arrive at (here $G(\\tau) = R(\\tau)$ and $R(\\tau)$ is the notation used in Lecture notes). A naive collary is the using causality to change that to return-to-go:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_\\theta J(\\theta)\n",
    "&= \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,G_t(\\tau)\\right]\n",
    "\\end{aligned} \\tag{F2} \n",
    "$$\n",
    "\n",
    "where $G_t=\\sum_{k=t}^{T-1}\\gamma^{k-t}r(s_k,a_k)$ and $d^{\\pi_\\theta}$ is the discounted state-visitation distribution. Next, we observe that \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{a_t\\sim\\pi_\\theta}\\!\\left[\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,b(s_t)\\right]=0,\n",
    "$$\n",
    "\n",
    "<!-- This is because condition on $s_t$, $b(s_t)$ is a constant inside expectation, and $\\mathbb{E}_{a_t\\sim\\pi_\\theta}\\!\\left[\\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\right]=0$.  -->\n",
    "\n",
    "Plug in the \"baseline\" $b(s_t)$ into the policy gradient gives us\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,(G_t(\\tau)- b(s_t))\\right] \\tag{F3}\n",
    "$$\n",
    "\n",
    "In practice most of the time people use the learned value function for the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient theorem (extended)\n",
    "\n",
    "Next we talk about another 3 chosen of policy gradient:\n",
    "\n",
    "From (F2) gradient we can easily see that $Q(s_t,a_t) = \\mathbb{E}[G_t(\\tau)]$, so plug in F2 gives us\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,Q(s_t,a_t)\\right] \\tag{F4}\n",
    "$$\n",
    "\n",
    "And followed by previous explanation of baseline, we can define $A(s_t,a_t) = Q(s_t,a_t) - V(s_t)$, thus we arrive the *advantage function* gradient.\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,A(s_t,a_t)\\right] \\tag{F5}\n",
    "$$\n",
    "\n",
    "The last formulation is by observing that\n",
    "$$\n",
    "\\mathbb{E}[Q(s_t,a_t)] = \\mathbb{E}[r(s_t,a_t) + \\gamma V(s_{t+1})]\n",
    "$$\n",
    "\n",
    "apply baseline to it gives us\n",
    "$$\n",
    "\\mathbb{E}_{\\tau\\sim p_\\theta}\\!\\left[\\sum_{t=0}^{T-1}\\gamma^t \\nabla_\\theta\\log\\pi_\\theta(a_t\\mid s_t)\\,(r(s_t,a_t) + \\gamma V(s_{t+1}) - V(s_t))\\right] \\tag{F6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convergence of Different Policy-Gradient Estimators\n",
    "\n",
    "We study a random walk on a ring. Let $\\mathcal{S}=\\mathbb{Z}_N$ and $\\mathcal{A}=\\{L,R\\}$. The dynamics and rewards are\n",
    "$$\n",
    "s_{t+1}\\equiv s_t+\\begin{cases}\n",
    "+1 & \\text{if }a_t = R, \\\\\n",
    "-1 & \\text{if }a_t = L,\n",
    "\\end{cases}\\ (\\mathrm{mod}\\,N),\\qquad\n",
    "r_t \\equiv \\begin{cases}\n",
    "r_\\text{terminal} & \\text{if done} \\\\\n",
    "r_\\text{step} & \\text{otherwise}\n",
    "\\end{cases},\\quad \\gamma\\in(0,1).\n",
    "$$\n",
    "\n",
    "Because this is tabular, Bellman consistency (Eq. 1.21) yields a linear system $AV=b$ (as in PSET1, Problem 4). Solving gives the exact values $V$, and $Q$ follows by one-step lookahead.\n",
    "\n",
    "**TODO:**\n",
    "1. Implement a minimal MLP policy ($x=s/N\\to\\pi_\\theta(\\cdot\\mid s)$) with a Softmax output.  \n",
    "2. Implement six MC gradient estimators: REINFORCE, return-to-go, baseline with $V$, using $Q$, advantage $Q{-}V$, and TD-residual.  \n",
    "3. Plot per-parameter sample std and the running-mean error $\\|\\bar g_k-\\nabla_\\theta J\\|_2$ vs. episodes, plus $|\\bar J_k-J_{\\text{true}}|$.\n",
    "   $$\\bar g_k = \\frac{1}{k} \\sum_i^k g_i, \\quad \\bar J_k = \\frac{1}{k} \\sum_i^k J_i$$\n",
    "   \n",
    "4. Comment on what you see, and explain it intuitively.\n",
    "\n",
    "Note:\n",
    "\n",
    "Here we provide the function `build_system`, `get_V_and_J` and `get_Q` for calculate the true value / action value. `finite_difference_grad` for approximate the true objective / gradient by finite difference. And also `logp_single` and `score_matrix_batch` for calculate $\\nabla J_\\theta$ in a batched manner (You can also use for-loop, but that takes quite long run time). But feel free to use your own code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from typing import Tuple, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.func import functional_call, vmap, jacrev\n",
    "\n",
    "\n",
    "# ----------------- utilities -----------------\n",
    "def set_seed(seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "# ----------------- policy network (last layer Softmax) -----------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.1: Implement the policy network\n",
    "    #########################################\n",
    "    \n",
    "\n",
    "# ----------------- DP: exact J(θ) using policy p_s -----------------\n",
    "def build_system(model: PolicyNet, N: int, gamma: float,\n",
    "                 running_reward: float, terminal_reward: float):\n",
    "    \"\"\"\n",
    "    In tabular case, we could build A,b directly from the bellman's equations (eq 1.21 in lecture note).\n",
    "    Build linear system A V = b for states s=1..N-1.\n",
    "    Transition probabilities p_s come from the torch policy (Right prob).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        s_idx = torch.arange(1, N, dtype=torch.float32)\n",
    "        x = s_idx / float(N)                  # (N-1,)\n",
    "        pi = model(x)                         # (N-1, 2)\n",
    "        p = pi[:, 1].cpu().numpy()            # P(right | s)\n",
    "        q = (1.0 - p)                         # P(left  | s)\n",
    "\n",
    "    A = np.zeros((N-1, N-1), dtype=np.float64)\n",
    "    b = np.zeros(N-1, dtype=np.float64)\n",
    "\n",
    "    if N - 1 == 1:\n",
    "        A[0, 0] = 1.0\n",
    "        b[0] = terminal_reward\n",
    "        return A, b\n",
    "\n",
    "    # s = 1 (index 0)\n",
    "    A[0, 0] = 1.0\n",
    "    A[0, 1] = -gamma * p[0]\n",
    "    b[0] = q[0] * terminal_reward + p[0] * running_reward\n",
    "\n",
    "    # s = 2..N-2 (indices 1..N-3)\n",
    "    for s in range(2, N-1):\n",
    "        i = s - 1\n",
    "        A[i, i]   = 1.0\n",
    "        A[i, i-1] = -gamma * q[i]\n",
    "        A[i, i+1] = -gamma * p[i]\n",
    "        b[i]      = running_reward\n",
    "\n",
    "    # s = N-1 (index N-2)\n",
    "    i = N - 2\n",
    "    A[i, i]   = 1.0\n",
    "    A[i, i-1] = -gamma * q[i]\n",
    "    b[i]      = p[i] * terminal_reward + q[i] * running_reward\n",
    "    return A, b\n",
    "\n",
    "def get_V_and_J(model: PolicyNet, N: int, gamma: float,\n",
    "                running_reward: float, terminal_reward: float):\n",
    "    \"\"\"Solve A V = b; return V(s) for s=1..N-1 and uniform-start J.\"\"\"\n",
    "    A, b = build_system(model, N, gamma, running_reward, terminal_reward)\n",
    "    V = np.linalg.solve(A, b)\n",
    "    return V, float(V.mean())\n",
    "\n",
    "def get_Q(model: PolicyNet, N: int, gamma: float,\n",
    "          running_reward: float, terminal_reward: float):\n",
    "    \"\"\"\n",
    "    Q(s,a) via one-step lookahead using V from DP.\n",
    "    Returns Q for s=1..N-1 (shape (N-1, 2)).\n",
    "    \"\"\"\n",
    "    V, _ = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "    V_full = np.zeros(N + 1)\n",
    "    V_full[1:N] = V\n",
    "    s = np.arange(1, N, dtype=np.int64)\n",
    "    sL, sR = s - 1, s + 1\n",
    "    rL = np.where(sL == 0, terminal_reward, running_reward)\n",
    "    rR = np.where(sR == N, terminal_reward, running_reward)\n",
    "    Q = np.empty((N-1, 2), dtype=np.float64)\n",
    "    Q[:, 0] = rL + gamma * V_full[sL]\n",
    "    Q[:, 1] = rR + gamma * V_full[sR]\n",
    "    return Q\n",
    "\n",
    "# ----------------- Finite-difference gradient on θ (torch) -----------------\n",
    "def finite_difference_grad(model: PolicyNet, N: int, gamma: float,\n",
    "                           running_reward: float, terminal_reward: float,\n",
    "                           eps: float = 1e-4, relative: bool = False,\n",
    "                           scheme: str = 'central'):\n",
    "    \"\"\"\n",
    "    Finite-difference ∇θ J where θ is the concatenated torch parameter vector.\n",
    "    Supports central or forward difference. Optional relative step size.\n",
    "    \"\"\"\n",
    "    theta0 = parameters_to_vector(model.parameters()).detach().clone()\n",
    "    _, J0 = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "    grad = torch.zeros_like(theta0)\n",
    "\n",
    "    for i in range(theta0.numel()):\n",
    "        base = float(abs(theta0[i])) if relative else 1.0\n",
    "        h = eps * max(1.0, base)\n",
    "\n",
    "        if scheme.lower() == 'central':\n",
    "            th_p = theta0.clone(); th_p[i] += h\n",
    "            th_m = theta0.clone(); th_m[i] -= h\n",
    "            vector_to_parameters(th_p, model.parameters())\n",
    "            Jp = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            vector_to_parameters(th_m, model.parameters())\n",
    "            Jm = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            grad[i] = (Jp - Jm) / (2.0 * h)\n",
    "        elif scheme.lower() == 'forward':\n",
    "            th_p = theta0.clone(); th_p[i] += h\n",
    "            vector_to_parameters(th_p, model.parameters())\n",
    "            Jp = get_V_and_J(model, N, gamma, running_reward, terminal_reward)[1]\n",
    "            grad[i] = (Jp - J0) / h\n",
    "        else:\n",
    "            raise ValueError(\"scheme must be 'central' or 'forward'\")\n",
    "\n",
    "    # restore original params\n",
    "    vector_to_parameters(theta0, model.parameters())\n",
    "    return J0, grad.detach()\n",
    "\n",
    "# ----------------- MC gradient estimators (REINFORCE family) -----------------\n",
    "def mc_grad_estimators(model: PolicyNet, N: int, s0_batch: torch.Tensor, gamma: float,\n",
    "                       step_cost: float, terminal_reward: float,\n",
    "                       V: torch.Tensor = None, Q: torch.Tensor = None):\n",
    "    \"\"\"\n",
    "    We compute per-sample score vectors using autograd by calling backward()\n",
    "    on log π(a_t|s_t) to obtain ∇θ log π(a_t|s_t).\n",
    "\n",
    "    Returns (all numpy arrays):\n",
    "      g1..g6: (B, P) per-episode gradient samples; J: (B,)\n",
    "        g1: full-return REINFORCE\n",
    "        g2: return-to-go REINFORCE (via cumulative scores H)\n",
    "        g3: baseline with V(s_t)\n",
    "        g4: use Q(s_t, a_t)\n",
    "        g5: use Advantage A = Q − V\n",
    "        g6: use TD residual δ_t = r_t + γ V(s_{t+1}) − V(s_t)\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    names, base_params = zip(*list(model.named_parameters()))\n",
    "    # Detach so these are leaf tensors, then turn grad on (required by jacrev)\n",
    "    params = tuple(p.detach().requires_grad_(True) for p in base_params)\n",
    "    P = sum(p.numel() for p in params)\n",
    "\n",
    "    B = int(s0_batch.numel())\n",
    "    s    = s0_batch.to(device).clone()\n",
    "    done = (s == 0) | (s == N)\n",
    "\n",
    "    H  = torch.zeros(B, P, device=device)  # cumulative score per-episode\n",
    "    g1 = torch.zeros(B, P, device=device)\n",
    "    g2 = torch.zeros(B, P, device=device)\n",
    "    g3 = torch.zeros(B, P, device=device)\n",
    "    g4 = torch.zeros(B, P, device=device)\n",
    "    g5 = torch.zeros(B, P, device=device)\n",
    "    g6 = torch.zeros(B, P, device=device)\n",
    "    J  = torch.zeros(B,   device=device)\n",
    "    gpw= torch.ones (B,   device=device)  # γ^t\n",
    "\n",
    "    if V is not None:\n",
    "        V = V.to(device)     # shape N+1, suggest V[0]=V[N]=0\n",
    "    if Q is not None:\n",
    "        Q = Q.to(device)     # shape (N+1,2), with Q(0,.)=Q(N,.)=0 if you padded\n",
    "    \n",
    "    def logp_single(param_tensors, s_scalar: torch.Tensor, a_scalar: torch.Tensor):\n",
    "        # Build a param dict for functional_call\n",
    "        pmap = {n: t for n, t in zip(names, param_tensors)}\n",
    "        x = (s_scalar.float() / float(N)).view(1, 1)\n",
    "        probs = functional_call(model, pmap, (x,))    # (1,2)\n",
    "        # Differentiable action selection via gather (avoid data-dependent indexing pitfalls)\n",
    "        logp = probs.log().gather(1, a_scalar.long().view(1, 1)).squeeze()  # scalar\n",
    "        return logp\n",
    "\n",
    "    # Note: you may found this function useful, this calculate ∇θ log π(a_i|s_i) in a batch manner\n",
    "    def score_matrix_batch(active_s: torch.Tensor, active_a: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns (B_act, P) where each row is ∇θ log π(a_i|s_i),\n",
    "        computed efficiently via vmap(jacrev) over (s,a).\n",
    "        \"\"\"\n",
    "        # jac is a pytree of tensors matching param shapes, each with leading dim B_act\n",
    "        jac = vmap(jacrev(logp_single), in_dims=(None, 0, 0))(params, active_s, active_a)\n",
    "        # Flatten each param’s jacobian and concatenate along feature dim\n",
    "        parts = [g.reshape(g.shape[0], -1) for g in jac]\n",
    "        return torch.cat(parts, dim=1)  # (B_act, P)\n",
    "\n",
    "\n",
    "    while not torch.all(done):\n",
    "        idx = (~done).nonzero(as_tuple=False).squeeze(1)\n",
    "        if idx.numel() == 0:\n",
    "            break\n",
    "\n",
    "        # Sample actions for all active states in one forward pass\n",
    "        x = (s[idx].float() / float(N)).unsqueeze(1)\n",
    "        with torch.no_grad():\n",
    "            probs = model(x)                            # (B_act, 2)\n",
    "            a = torch.multinomial(probs, 1).squeeze(1) # (B_act,)\n",
    "\n",
    "        # ∇θ log π(a|s): (B_act, P) using backward()\n",
    "        score_mat = score_matrix_batch(s[idx], a)\n",
    "\n",
    "        # Next state, termination, and rewards\n",
    "        s_next   = s[idx] + torch.where(a == 1, 1, -1)\n",
    "        term_any = (s_next == 0) | (s_next == N)\n",
    "        r_t = torch.where(\n",
    "            term_any,\n",
    "            torch.tensor(terminal_reward, device=device),\n",
    "            torch.tensor(step_cost, device=device)\n",
    "        ).float()\n",
    "        \n",
    "        #########################################\n",
    "        #TODO 2.1: Implement the six policy gradient estimators\n",
    "        #########################################\n",
    "\n",
    "        \n",
    "    return (g1.detach().cpu().numpy(),\n",
    "            g2.detach().cpu().numpy(),\n",
    "            (g3.detach().cpu().numpy() if V is not None else np.zeros_like(g2.detach().cpu().numpy())),\n",
    "            (g4.detach().cpu().numpy() if Q is not None else np.zeros_like(g2.detach().cpu().numpy())),\n",
    "            (g5.detach().cpu().numpy() if (V is not None and Q is not None) else np.zeros_like(g2.detach().cpu().numpy())),\n",
    "            g6.detach().cpu().numpy() if V is not None else np.zeros_like(g2.detach().cpu().numpy()),\n",
    "            J.detach().cpu().numpy())\n",
    "\n",
    "# ----------------- main -----------------\n",
    "set_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Environment\n",
    "N = 10\n",
    "gamma = 0.9\n",
    "running_reward = -1.0\n",
    "terminal_reward = 100.0\n",
    "\n",
    "# Policy\n",
    "model = PolicyNet(hidden=2).to(device)\n",
    "\n",
    "# Exact J and FD gradient\n",
    "J_true, grad_fd = finite_difference_grad(\n",
    "    model, N, gamma, running_reward, terminal_reward,\n",
    "    eps=1e-4, relative=True, scheme='central'\n",
    ")\n",
    "\n",
    "# Baselines from DP value V(s) and Q(s,a)\n",
    "V_np, _ = get_V_and_J(model, N, gamma, running_reward, terminal_reward)\n",
    "V = torch.tensor(np.concatenate(([0.0], V_np, [0.0])), dtype=torch.float32, device=device)  # V(0)=V(N)=0\n",
    "\n",
    "Q_np = get_Q(model, N, gamma, running_reward, terminal_reward).astype(np.float32)\n",
    "Q_t  = torch.tensor(Q_np, dtype=torch.float32, device=device)\n",
    "zero_row = torch.zeros(1, 2, dtype=torch.float32, device=device)\n",
    "Q = torch.cat([zero_row, Q_t, zero_row], dim=0)  # Q(0,.)=Q(N,.)=0\n",
    "\n",
    "# Monte Carlo (batched episodes)\n",
    "episodes = 20000  # adjust as needed\n",
    "s0_batch = torch.randint(1, N, (episodes,), dtype=torch.int64, device=device)\n",
    "g1, g2, g3, g4, g5, g6, J = mc_grad_estimators(\n",
    "    model, N, s0_batch, gamma,\n",
    "    step_cost=running_reward,\n",
    "    terminal_reward=terminal_reward,\n",
    "    V=V, Q=Q\n",
    ")\n",
    "\n",
    "#########################################\n",
    "#TODO 2.1: Plot your result here\n",
    "# 1. Print out the standard deviation of each gradient estimator\n",
    "# 2. Plot the running error of the estimated J vs the true J, you may found np.cumsum(:, axis=0) / np.arange(1, len(J) + 1) useful\n",
    "# 3. Plot the running error of each gradient estimator vs the FD gradient\n",
    "#########################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 REINFORCE algorithm\n",
    "\n",
    "**Algorithm Recap — REINFORCE (Monte-Carlo Policy Gradient)**\n",
    "\n",
    "1. **Policy network**  \n",
    "   Stochastic policy $\\pi_\\theta(a\\mid s)$ \n",
    "\n",
    "2. **Trajectory sampling**  \n",
    "   Roll out episodes with $\\pi_\\theta$: $(s_1,a_1,r_1,\\dots,s_T,a_T,r_T)$.\n",
    "\n",
    "3. **Returns / advantages**\n",
    "   - Monte-Carlo return:\n",
    "     $$\n",
    "     G_t=\\sum_{t'=t}^{T}\\gamma^{\\,t'-t} r_{t'}.\n",
    "     $$\n",
    "   - Advantage: $A_t = G_t - b(s_t)$.\n",
    "\n",
    "4. **Policy-gradient update**\n",
    "   - Estimator:\n",
    "     $$\n",
    "     \\hat g(\\theta)=\\frac{1}{N}\\sum_{i=1}^{N}\\sum_{t=1}^{T_i}\n",
    "       \\nabla_\\theta \\log \\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\, \\gamma^t A_t^{(i)}.\n",
    "     $$\n",
    "   - Gradient ascent: $\\theta \\leftarrow \\theta + \\alpha\\,\\hat g(\\theta)$.\n",
    "\n",
    "5. **Learned value baseline (optional)**\n",
    "   - Regress $V_\\psi(s)$ to returns:\n",
    "     $$\n",
    "     \\min_\\psi \\frac{1}{N}\\sum_{i,t}\\big(V_\\psi(s_t^{(i)})-G_t^{(i)}\\big)^2,\n",
    "     \\qquad A_t\\!=\\!G_t\\!-\\!V_\\psi(s_t).\n",
    "     $$\n",
    "\n",
    "6. **Mini-batch training**\n",
    "   - Collect $N$ episodes (or $M$ steps), compute $G_t/A_t$; optimize\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{PG}}(\\theta)=\n",
    "     -\\frac{1}{N}\\sum_{i,t}\\log\\pi_\\theta(a_t^{(i)}\\mid s_t^{(i)})\\,A_t^{(i)}.\n",
    "     $$\n",
    "\n",
    "**TODO:**\n",
    "- implement policy net and value net\n",
    "- implement the main algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Acrobot-v1\"\n",
    "SEED = 0\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.995\n",
    "\n",
    "LR_POLICY = 3e-4\n",
    "LR_VALUE  = 1e-3\n",
    "MAX_EPOCHS = 400\n",
    "BATCH_SIZE = 16\n",
    "MAX_EP_LEN = 1000\n",
    "VALUE_UPDATES = 10\n",
    "\n",
    "GRAD_CLIP = 10.0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]   # 6 for Acrobot\n",
    "act_dim = env.action_space.n               # 3 for Acrobot\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# ------------------- tensor wrapper (given as a reference) -------------------\n",
    "class TWrapper(gym.Wrapper):\n",
    "    def __init__(self, env): super().__init__(env)\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = self.env.reset(seed=seed, options=options)\n",
    "        return torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0), info\n",
    "    def step(self, action):\n",
    "        a = int(action.item()) if isinstance(action, torch.Tensor) else int(action)\n",
    "        obs, r, term, trunc, info = self.env.step(a)\n",
    "        done = bool(term or trunc)\n",
    "        obs_t = torch.from_numpy(np.asarray(obs, np.float32)).unsqueeze(0)\n",
    "        r_t   = torch.tensor([[r]], dtype=torch.float32)\n",
    "        d_t   = torch.tensor([[done]], dtype=torch.bool)\n",
    "        return obs_t, r_t, d_t, info\n",
    "    \n",
    "env = TWrapper(env)\n",
    "\n",
    "# ------------------- discrete policy net (given as a reference) -------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement policy network\n",
    "    #########################################\n",
    "    \n",
    "# ------------------- value baseline (given as a reference) -------------------\n",
    "class ValueNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement value network\n",
    "    #########################################\n",
    "    \n",
    "    \n",
    "policy = PolicyNet(obs_dim=obs_dim, hidden=HIDDEN, act_dim=act_dim).to(DEVICE)\n",
    "vnet   = ValueNet(obs_dim=obs_dim, hidden=HIDDEN).to(DEVICE)\n",
    "\n",
    "# ------------------- utils -------------------\n",
    "def mc_returns_single_traj(R: torch.Tensor, gamma: float) -> torch.Tensor:\n",
    "    \"\"\"R: [T] -> G: [T], reverse within a single trajectory.\"\"\"\n",
    "    G = torch.zeros_like(R)\n",
    "    running = 0.0\n",
    "    for t in range(R.numel() - 1, -1, -1):\n",
    "        running = R[t] + gamma * running\n",
    "        G[t] = running\n",
    "    return G\n",
    "\n",
    "# ------------------- training -------------------\n",
    "def train():\n",
    "    #########################################\n",
    "    #TODO 2.2: Implement vanilla REINFORCE algorithm\n",
    "    #########################################\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "    return policy, vnet, returns_history\n",
    "\n",
    "policy, vnet, returns_history = train()\n",
    "\n",
    "def eval(policy, episodes=10, greedy=True, device=DEVICE, max_len=MAX_EP_LEN):\n",
    "    env = gym.make(\"Acrobot-v1\")\n",
    "    policy.eval()\n",
    "    succ, max_hs = [], []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(episodes):\n",
    "            o, _ = env.reset()\n",
    "            ok, m = False, -1e9\n",
    "            for _ in range(max_len):\n",
    "                s = torch.as_tensor(o, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                logits = policy(s)\n",
    "                a = int(logits.argmax(-1)) if greedy else int(Categorical(logits=logits).sample())\n",
    "                o, r, term, trunc, _ = env.step(a)\n",
    "                c1, s1, c2, s2 = o[:4]; m = max(m, float(-c1 - (c1*c2 - s1*s2)))  # tip height\n",
    "                if term or trunc: ok = bool(term); break\n",
    "            succ.append(ok); max_hs.append(m)\n",
    "    print(f\"success={np.mean(succ):.1%}, mean_max_tip={np.mean(max_hs):.3f}\")\n",
    "    \n",
    "eval(policy, episodes=100, greedy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Actor-critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCE with learned value function often have high variance (recall what we find in PSET2 2.1). Actor-critic method replace the advantage $A_t = G_t - b(s_t)$ with temporal-difference error $r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "**Algorithm recap**\n",
    "\n",
    "1. **Networks**\n",
    "   - **Actor**: stochastic policy $ \\pi_\\theta(a\\mid s) $.\n",
    "   - **Critic**: value $ V_\\psi(s) $\n",
    "  \n",
    "2. **Data collection**\n",
    "   \n",
    "   Roll out for $n$ steps (or full episodes) with $\\pi_\\theta$; store $(s_t,a_t,r_t,s_{t+1},\\text{done}_t)$.\n",
    "\n",
    "3. **TD advantage (one-step)**\n",
    "   $$\n",
    "   y_t = r_t + \\gamma \\,V_{\\psi}(s_{t+1}),\\qquad\n",
    "   \\delta_t = y_t - V_\\psi(s_t).\n",
    "   $$\n",
    "   Use $\\delta_t$ as **advantage** (variance lower than Monte-Carlo $G_t$).\n",
    "\n",
    "4. **Losses**\n",
    "   - **Actor**  \n",
    "     $$\n",
    "     \\mathcal L_{\\pi}(\\theta)\n",
    "     = -\\,\\mathbb E\\big[\\,\\log \\pi_\\theta(a_t\\!\\mid s_t)\\,\\delta_t \\big]\n",
    "     $$\n",
    "   - **Critic**  \n",
    "     $$\n",
    "     \\mathcal L_V(\\psi) = \\tfrac12\\,\\mathbb E\\big[(V_\\psi(s_t)-y_t)^2\\big].\n",
    "     $$\n",
    "\n",
    "Several other features you may consider:\n",
    "- Multi-step update for value function \n",
    "- Normalize the advantage over batch\n",
    "\n",
    "**TODO:**\n",
    "- implement policy net and value net\n",
    "- implement the main algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On-policy Actor–Critic for Acrobot-v1\n",
    "# - Discrete actions, update every K steps (no need to finish episodes)\n",
    "\n",
    "import math, random\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Acrobot-v1\"\n",
    "SEED = 0\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.995\n",
    "\n",
    "LR_POLICY = 3e-4\n",
    "LR_VALUE  = 1e-3\n",
    "MAX_EPOCHS = 500\n",
    "STEPS_PER_UPDATE = 64           # ← collect this many steps, then update (true on-policy)\n",
    "\n",
    "CRITIC_UPDATES = 1              # critic updates per actor step\n",
    "GRAD_CLIP = 10.0\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]   # 6 for Acrobot\n",
    "act_dim = env.action_space.n               # 3 for Acrobot\n",
    "\n",
    "np.random.seed(SEED); random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "# ------------------- networks -------------------\n",
    "class PolicyNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement policy network\n",
    "    #########################################\n",
    "    \n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement value network\n",
    "    #########################################\n",
    "    \n",
    "\n",
    "policy = PolicyNet(obs_dim, HIDDEN, act_dim).to(DEVICE)\n",
    "value  = ValueNet(obs_dim, HIDDEN).to(DEVICE)\n",
    "opt_pi = torch.optim.Adam(policy.parameters(), lr=LR_POLICY)\n",
    "opt_v  = torch.optim.Adam(value.parameters(),  lr=LR_VALUE)\n",
    "\n",
    "# ------------------- helper -------------------\n",
    "@torch.no_grad()\n",
    "def to_t(s): return torch.as_tensor(s, dtype=torch.float32, device=DEVICE).unsqueeze(0)\n",
    "\n",
    "# ------------------- training (A2C / 1-step actor-critic) -------------------\n",
    "def train():\n",
    "    returns_history: List[float] = []\n",
    "    ep_ret, ep_len = 0.0, 0\n",
    "    obs, _ = env.reset(seed=SEED)\n",
    "    #########################################\n",
    "    #TODO 3.1: Implement the main algorithm\n",
    "    #########################################\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "    return policy, value, returns_history\n",
    "\n",
    "policy, value, returns = train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: PPO for pendulum\n",
    "\n",
    "Vanilla actor-critic often face the problem of distribution shift. Advanced actor-critic deal with this problem by adding trust region constraints. PPO is the most famous and widely-used one in robotics. In this problem you will implement PPO on gym's `pendulum-v1` environment.\n",
    "\n",
    "### Environment & action space\n",
    "\n",
    "- **Env**: `Pendulum-v1` (pendulum swing-up) [Link](https://gymnasium.farama.org/environments/classic_control/pendulum/)\n",
    "- **Observation**: 3-D vector $[\\cos\\theta,\\ \\sin\\theta,\\ \\dot\\theta]$.\n",
    "- **Actions**: Continuous torque, shape $(1,)$, range $[-2, 2]$ (env clips to bounds).\n",
    "- **Reward**: \n",
    "  $$ r = -\\big(\\theta^2 + 0.1\\,\\dot\\theta^{\\,2} + 0.001\\,u^{2}\\big) $$\n",
    "  where $\\theta\\in(-\\pi,\\pi]$ is angle to upright ($0$ is upright), $\\dot\\theta$ is angular velocity, and $u$ is applied torque. Maximized when the pendulum is upright and still with minimal torque.\n",
    "\n",
    "\n",
    "### Algorithm Recap\n",
    "\n",
    "**Policy & Value.**\n",
    "- Policy: Gaussian $\\mathcal{N}(\\mu_\\theta(s), \\sigma_\\theta(s))$.\n",
    "- Critic: scalar value $V_\\phi(s)$.\n",
    "\n",
    "**Data collection (on-policy).**\n",
    "- Roll out episodes using the current policy, storing $(s_t,a_t,r_t,s_{t+1},d_t)$.\n",
    "\n",
    "**Targets and Advantage.**\n",
    "- One-step TD target: $\\hat{V}_t = r_t + \\gamma V_\\phi(s_{t+1})$.\n",
    "- TD residual: $\\delta_t = \\hat{V}_t - V_\\phi(s_t)$.\n",
    "- GAE($\\lambda$) advantage:\n",
    "  $$\n",
    "  \\hat{A}_t = \\sum_{k=0}^{\\infty} (\\gamma\\lambda)^k \\, \\delta_{t+k}.\n",
    "  $$\n",
    "  (Computed by a backward recursion.)\n",
    "\n",
    "**PPO-Clip objective.**\n",
    "- Log-ratio $r_t(\\theta) = \\frac{\\pi_\\theta(a_t\\mid s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t\\mid s_t)}$.\n",
    "- Clipped surrogate:\n",
    "  $$\n",
    "  \\mathcal{L}^{\\text{CLIP}}(\\theta)\n",
    "  = \\mathbb{E}\\Big[\\min\\big(r_t(\\theta)\\hat{A}_t,\\ \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\big)\\Big].\n",
    "  $$\n",
    "- Value loss: $\\mathcal{L}_V = \\|V_\\phi(s_t) - \\hat{V}_t\\|_2^2$.\n",
    "- Total loss (per minibatch): $-\\mathcal{L}^{\\text{CLIP}} + c_v \\mathcal{L}_V$ (entropy term optional).\n",
    "\n",
    "**Update.**\n",
    "- Cache old log-probs once per batch.\n",
    "- For several **epochs**, shuffle the batch and optimize the total loss on minibatches (Adam).\n",
    "\n",
    "**TODO:** Implement a complete PPO agent from scratch, using the provided scaffold and suggested hyperparameters as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math, random\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "# ------------------- hyperparameters -------------------\n",
    "ENV_ID = \"Pendulum-v1\"\n",
    "SEED = 0\n",
    "\n",
    "LR_POLICY = 1e-4\n",
    "LR_VALUE  = 5e-3\n",
    "NUM_EPSIODE = 3000       # (kept your variable name)\n",
    "HIDDEN = 128\n",
    "GAMMA = 0.99\n",
    "LAMBDA = 0.9\n",
    "VF_COEF = 0.9            # value loss weight in the total loss\n",
    "UPDATE_EPOCHS = 10       # PPO epochs per update\n",
    "CLIP_EPS = 0.2           # PPO clipping epsilon\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# ------------------- env & seeding -------------------\n",
    "env = gym.make(ENV_ID)\n",
    "env.reset(seed=SEED)\n",
    "env.action_space.seed(SEED)\n",
    "env.observation_space.seed(SEED)\n",
    "\n",
    "state_dim  = env.observation_space.shape[0]   # 3 for Pendulum\n",
    "action_dim = env.action_space.shape[0]        # 1 for Pendulum\n",
    "\n",
    "#########################################\n",
    "#TODO 4: Implement PPO\n",
    "#########################################\n",
    "\n",
    "# ------------------- utils -------------------\n",
    "def compute_advantage(gamma: float, lmbda: float, td_delta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Pure torch GAE-style backward recursion to avoid NumPy conversions.\n",
    "    td_delta: [T,1] or [T]; returns [T,1].\n",
    "    \"\"\"\n",
    "    td = td_delta.view(-1)                # [T]\n",
    "    adv = torch.zeros_like(td)\n",
    "    gae = torch.zeros(1, dtype=td.dtype, device=td.device)\n",
    "    for t in range(td.shape[0] - 1, -1, -1):\n",
    "        gae = gamma * lmbda * gae + td[t]\n",
    "        adv[t] = gae\n",
    "    return adv.view(-1, 1)\n",
    "\n",
    "# ------------------- PPO (continuous) -------------------\n",
    "class PPOContinuous:\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, vf_coef, gamma, device):\n",
    "        self.actor  = PolicyNetContinuous(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer  = torch.optim.Adam(self.actor.parameters(),  lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.gamma  = gamma\n",
    "        self.lmbda  = lmbda\n",
    "        self.epochs = epochs\n",
    "        self.eps    = eps\n",
    "        self.vf_coef = vf_coef\n",
    "        self.device = device\n",
    "\n",
    "# ------------------- training loop (Gymnasium API) -------------------\n",
    "def train_on_policy_agent(env, agent, num_episodes):\n",
    "    \n",
    "\n",
    "# ------------------- run -------------------\n",
    "agent = PPOContinuous(state_dim, HIDDEN, action_dim, LR_POLICY, LR_VALUE,\n",
    "                      LAMBDA, UPDATE_EPOCHS, CLIP_EPS, VF_COEF, GAMMA, DEVICE)\n",
    "return_list = train_on_policy_agent(env, agent, NUM_EPSIODE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Mujoco Half-cheetch envornment with stable baseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you will use gym's [Mujoco](https://gymnasium.farama.org/environments/mujoco/) environment and [stable baseline3](https://stable-baselines3.readthedocs.io/en/master/) to train a PPO network on Half-cheetah environment.\n",
    "\n",
    "### Half-cheetah\n",
    "This environment is based on the work of P. Wawrzyński in “A Cat-Like Robot Real-Time Learning to Run”. The HalfCheetah is a 2-dimensional robot consisting of 9 body parts and 8 joints connecting them (including two paws). The goal is to apply torque to the joints to make the cheetah run forward (right) as fast as possible, with a positive reward based on the distance moved forward and a negative reward for moving backward. \n",
    "\n",
    "Download it using `pip install \"gymnasium[mujoco]\"`\n",
    "\n",
    "### Stable baseline 3\n",
    "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. You can directly load `PPO` module from the repo and define the hyper-parameter yourselves.\n",
    "\n",
    "Download it using `pip install 'stable-baselines3[extra]'`\n",
    "\n",
    "**TODO:** Tune the parameter yourself, what's your feeling about different parameters?\n",
    "\n",
    "Note: the output is printed in the `logs/progress.csv` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "save = \"ckpt/half_cheetah_ppo\"\n",
    "\n",
    "env = Monitor(gym.make(\"HalfCheetah-v4\"))\n",
    "\n",
    "#########################################\n",
    "#TODO 5: Change the parameter yourself to finish training\n",
    "#########################################\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=dict(\n",
    "        log_std_init=-2,\n",
    "        ortho_init=False,\n",
    "        activation_fn=nn.ReLU,\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256]),\n",
    "    ),\n",
    "    # PPO clipping parameter\n",
    "    clip_range=0.2,\n",
    "    # entropy coefficient\n",
    "    ent_coef=0.0004,\n",
    "    # GAE lambda parameter\n",
    "    gae_lambda=0.92,\n",
    "    gamma=0.98,\n",
    "    learning_rate=2.5e-5,\n",
    "    max_grad_norm=0.8,\n",
    "    n_steps=int(512*4),\n",
    "    # number of epochs when optimizing one batch\n",
    "    n_epochs=20,\n",
    "    device=\"cpu\",\n",
    "    # value function coefficient in the loss\n",
    "    vf_coef=0.5,\n",
    "    verbose=1,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "new_logger = configure(\"logs\", [\"csv\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "n_envs = model.n_envs     \n",
    "n_steps = model.n_steps   \n",
    "total_ts = 500 * n_steps * n_envs\n",
    "\n",
    "print(\"Starting learning...\")\n",
    "# This can take around 10 minutes on a Mac laptop\n",
    "model.learn(total_ts, log_interval=10)\n",
    "print(\"Learning finished.\")\n",
    "model.save(save)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.logger import configure\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "save = \"ckpt/half_cheetah_ppo\"\n",
    "\n",
    "# Load and test saved model\n",
    "import time\n",
    "env = gym.make(\"HalfCheetah-v4\", render_mode=\"human\")\n",
    "env.reset()\n",
    "# env = gym.make(\"racetrack-fast-v0\", render_mode=\"rgb_array\")\n",
    "model = PPO.load(save)\n",
    "\n",
    "while True:\n",
    "  done = truncated = False\n",
    "  obs, info = env.reset()\n",
    "  while not (done or truncated):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, truncated, info = env.step(action)\n",
    "    time.sleep(0.1)\n",
    "  # env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
